#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•RLç»„ä»¶æµ‹è¯•

ç›´æ¥æµ‹è¯•å•ä¸ªRLç»„ä»¶ï¼Œä¸ä¾èµ–ä»»ä½•å¤–éƒ¨æ¨¡å—ã€‚

Author: AgenticX Team
Date: 2025
"""

import torch
import numpy as np
from datetime import datetime
from collections import deque

print("=== ç®€å•RLç»„ä»¶åŠŸèƒ½éªŒè¯ ===")

# æµ‹è¯•1: åŸºç¡€æ•°æ®ç»“æ„
print("\n1. æµ‹è¯•åŸºç¡€æ•°æ®ç»“æ„...")
try:
    # æ¨¡æ‹ŸExperienceæ•°æ®ç»“æ„
    class SimpleExperience:
        def __init__(self, state, action, reward, next_state, done, agent_id):
            self.state = state
            self.action = action
            self.reward = reward
            self.next_state = next_state
            self.done = done
            self.agent_id = agent_id
            self.timestamp = datetime.now()
    
    # åˆ›å»ºæµ‹è¯•ç»éªŒ
    exp = SimpleExperience(
        state=torch.rand(256),
        action=torch.randint(0, 10, (1,)),
        reward=0.5,
        next_state=torch.rand(256),
        done=False,
        agent_id="test_agent"
    )
    
    assert exp.state.shape == (256,)
    assert exp.reward == 0.5
    assert exp.agent_id == "test_agent"
    
    print("  âœ“ åŸºç¡€æ•°æ®ç»“æ„æµ‹è¯•é€šè¿‡")
except Exception as e:
    print(f"  âœ— åŸºç¡€æ•°æ®ç»“æ„æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•2: ç®€å•ç­–ç•¥ç½‘ç»œ
print("\n2. æµ‹è¯•ç®€å•ç­–ç•¥ç½‘ç»œ...")
try:
    import torch.nn as nn
    import torch.nn.functional as F
    
    class SimplePolicyNetwork(nn.Module):
        def __init__(self, state_dim, action_dim):
            super().__init__()
            self.fc1 = nn.Linear(state_dim, 128)
            self.fc2 = nn.Linear(128, 64)
            self.policy_head = nn.Linear(64, action_dim)
            self.value_head = nn.Linear(64, 1)
        
        def forward(self, state):
            x = F.relu(self.fc1(state))
            x = F.relu(self.fc2(x))
            
            action_logits = self.policy_head(x)
            value = self.value_head(x)
            
            return {
                'action_logits': action_logits,
                'value': value,
                'action_probs': F.softmax(action_logits, dim=-1)
            }
    
    # åˆ›å»ºå’Œæµ‹è¯•ç­–ç•¥ç½‘ç»œ
    policy = SimplePolicyNetwork(256, 10)
    test_state = torch.rand(1, 256)
    
    output = policy(test_state)
    
    assert 'action_logits' in output
    assert 'value' in output
    assert 'action_probs' in output
    assert output['action_probs'].shape == (1, 10)
    
    print(f"  âœ“ ç­–ç•¥ç½‘ç»œæµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output['action_probs'].shape}")
except Exception as e:
    print(f"  âœ— ç­–ç•¥ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•3: ç»éªŒç¼“å†²åŒº
print("\n3. æµ‹è¯•ç»éªŒç¼“å†²åŒº...")
try:
    class SimpleExperienceBuffer:
        def __init__(self, capacity=1000):
            self.capacity = capacity
            self.buffer = deque(maxlen=capacity)
        
        def add(self, experience):
            self.buffer.append(experience)
        
        def sample(self, batch_size):
            if len(self.buffer) < batch_size:
                return list(self.buffer)
            
            indices = np.random.choice(len(self.buffer), batch_size, replace=False)
            return [self.buffer[i] for i in indices]
        
        def __len__(self):
            return len(self.buffer)
    
    # åˆ›å»ºç¼“å†²åŒºå¹¶æ·»åŠ ç»éªŒ
    buffer = SimpleExperienceBuffer(100)
    
    for i in range(20):
        exp = SimpleExperience(
            state=torch.rand(256),
            action=torch.randint(0, 10, (1,)),
            reward=np.random.uniform(-1, 1),
            next_state=torch.rand(256),
            done=i % 10 == 9,
            agent_id=f"agent_{i % 4}"
        )
        buffer.add(exp)
    
    # æµ‹è¯•é‡‡æ ·
    sampled = buffer.sample(5)
    assert len(sampled) == 5
    assert len(buffer) == 20
    
    print(f"  âœ“ ç»éªŒç¼“å†²åŒºæµ‹è¯•é€šè¿‡ï¼Œå¤§å°: {len(buffer)}")
except Exception as e:
    print(f"  âœ— ç»éªŒç¼“å†²åŒºæµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•4: å¥–åŠ±è®¡ç®—
print("\n4. æµ‹è¯•å¥–åŠ±è®¡ç®—...")
try:
    class SimpleRewardCalculator:
        def __init__(self):
            self.weights = {
                'task_completion': 0.4,
                'efficiency': 0.3,
                'safety': 0.3
            }
        
        def calculate_reward(self, context):
            reward = 0.0
            
            # ä»»åŠ¡å®Œæˆå¥–åŠ±
            if context.get('task_success', False):
                reward += 1.0 * self.weights['task_completion']
            
            # æ•ˆç‡å¥–åŠ±
            efficiency = context.get('efficiency', 0.5)
            reward += efficiency * self.weights['efficiency']
            
            # å®‰å…¨å¥–åŠ±
            safety_score = 1.0 - context.get('error_count', 0) * 0.1
            reward += max(0, safety_score) * self.weights['safety']
            
            return reward
    
    # æµ‹è¯•å¥–åŠ±è®¡ç®—
    calculator = SimpleRewardCalculator()
    
    context1 = {
        'task_success': True,
        'efficiency': 0.8,
        'error_count': 0
    }
    
    context2 = {
        'task_success': False,
        'efficiency': 0.3,
        'error_count': 2
    }
    
    reward1 = calculator.calculate_reward(context1)
    reward2 = calculator.calculate_reward(context2)
    
    assert reward1 > reward2  # å¥½çš„ä¸Šä¸‹æ–‡åº”è¯¥æœ‰æ›´é«˜çš„å¥–åŠ±
    assert 0 <= reward1 <= 2  # å¥–åŠ±åœ¨åˆç†èŒƒå›´å†…
    
    print(f"  âœ“ å¥–åŠ±è®¡ç®—æµ‹è¯•é€šè¿‡ï¼Œå¥–åŠ±1: {reward1:.3f}, å¥–åŠ±2: {reward2:.3f}")
except Exception as e:
    print(f"  âœ— å¥–åŠ±è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•5: çŠ¶æ€ç¼–ç 
print("\n5. æµ‹è¯•çŠ¶æ€ç¼–ç ...")
try:
    class SimpleStateEncoder(nn.Module):
        def __init__(self, image_dim=224*224*3, text_dim=512, action_dim=50, output_dim=256):
            super().__init__()
            self.image_encoder = nn.Linear(image_dim, 128)
            self.text_encoder = nn.Linear(text_dim, 128)
            self.action_encoder = nn.Linear(action_dim, 128)
            self.fusion = nn.Linear(384, output_dim)  # 3 * 128
        
        def forward(self, image, text, action_history):
            # ç¼–ç å„ä¸ªæ¨¡æ€
            image_feat = F.relu(self.image_encoder(image.flatten(1)))
            text_feat = F.relu(self.text_encoder(text))
            action_feat = F.relu(self.action_encoder(action_history.flatten(1)))
            
            # èåˆç‰¹å¾
            combined = torch.cat([image_feat, text_feat, action_feat], dim=1)
            output = self.fusion(combined)
            
            return output
    
    # æµ‹è¯•çŠ¶æ€ç¼–ç 
    encoder = SimpleStateEncoder()
    
    # æ¨¡æ‹Ÿè¾“å…¥
    image = torch.rand(1, 3, 224, 224)
    text = torch.rand(1, 512)
    action_history = torch.rand(1, 10, 5)
    
    encoded_state = encoder(image, text, action_history)
    
    assert encoded_state.shape == (1, 256)
    
    print(f"  âœ“ çŠ¶æ€ç¼–ç æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {encoded_state.shape}")
except Exception as e:
    print(f"  âœ— çŠ¶æ€ç¼–ç æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•6: ç­–ç•¥æ›´æ–°
print("\n6. æµ‹è¯•ç­–ç•¥æ›´æ–°...")
try:
    # åˆ›å»ºç­–ç•¥å’Œä¼˜åŒ–å™¨
    policy = SimplePolicyNetwork(256, 10)
    optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    states = torch.rand(32, 256)
    actions = torch.randint(0, 10, (32,))
    rewards = torch.rand(32)
    
    # å‰å‘ä¼ æ’­
    outputs = policy(states)
    action_probs = outputs['action_probs']
    values = outputs['value'].squeeze()
    
    # è®¡ç®—æŸå¤±
    action_log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze()
    policy_loss = -(action_log_probs * rewards).mean()
    value_loss = F.mse_loss(values, rewards)
    total_loss = policy_loss + 0.5 * value_loss
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    
    assert total_loss.item() > 0
    
    print(f"  âœ“ ç­–ç•¥æ›´æ–°æµ‹è¯•é€šè¿‡ï¼ŒæŸå¤±: {total_loss.item():.3f}")
except Exception as e:
    print(f"  âœ— ç­–ç•¥æ›´æ–°æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•7: å¤šæ™ºèƒ½ä½“åè°ƒ
print("\n7. æµ‹è¯•å¤šæ™ºèƒ½ä½“åè°ƒ...")
try:
    class MultiAgentCoordinator:
        def __init__(self):
            self.agents = {
                'manager': SimplePolicyNetwork(256, 10),
                'executor': SimplePolicyNetwork(256, 10),
                'reflector': SimplePolicyNetwork(256, 10),
                'notetaker': SimplePolicyNetwork(256, 10)
            }
            self.shared_buffer = SimpleExperienceBuffer(1000)
        
        def collect_experiences(self, num_steps=10):
            experiences = []
            
            for step in range(num_steps):
                for agent_id, policy in self.agents.items():
                    # æ¨¡æ‹Ÿç»éªŒæ”¶é›†
                    state = torch.rand(256)
                    
                    with torch.no_grad():
                        output = policy(state.unsqueeze(0))
                        action_probs = output['action_probs']
                        action = torch.multinomial(action_probs, 1).item()
                    
                    exp = SimpleExperience(
                        state=state,
                        action=torch.tensor([action]),
                        reward=np.random.uniform(-1, 1),
                        next_state=torch.rand(256),
                        done=step == num_steps - 1,
                        agent_id=agent_id
                    )
                    
                    experiences.append(exp)
                    self.shared_buffer.add(exp)
            
            return experiences
        
        def update_policies(self, experiences):
            # æŒ‰æ™ºèƒ½ä½“åˆ†ç»„ç»éªŒ
            agent_experiences = {}
            for exp in experiences:
                if exp.agent_id not in agent_experiences:
                    agent_experiences[exp.agent_id] = []
                agent_experiences[exp.agent_id].append(exp)
            
            update_results = {}
            
            for agent_id, agent_exps in agent_experiences.items():
                if len(agent_exps) >= 4:  # æœ€å°æ‰¹æ¬¡å¤§å°
                    policy = self.agents[agent_id]
                    optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)
                    
                    # ç®€åŒ–çš„ç­–ç•¥æ›´æ–°
                    states = torch.stack([exp.state for exp in agent_exps])
                    rewards = torch.tensor([exp.reward for exp in agent_exps])
                    
                    outputs = policy(states)
                    values = outputs['value'].squeeze()
                    
                    loss = F.mse_loss(values, rewards)
                    
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    
                    update_results[agent_id] = loss.item()
            
            return update_results
    
    # æµ‹è¯•å¤šæ™ºèƒ½ä½“åè°ƒ
    coordinator = MultiAgentCoordinator()
    
    # æ”¶é›†ç»éªŒ
    experiences = coordinator.collect_experiences(5)
    assert len(experiences) == 20  # 4ä¸ªæ™ºèƒ½ä½“ * 5æ­¥
    
    # æ›´æ–°ç­–ç•¥
    update_results = coordinator.update_policies(experiences)
    assert len(update_results) > 0
    
    print(f"  âœ“ å¤šæ™ºèƒ½ä½“åè°ƒæµ‹è¯•é€šè¿‡ï¼Œæ›´æ–°äº† {len(update_results)} ä¸ªæ™ºèƒ½ä½“")
except Exception as e:
    print(f"  âœ— å¤šæ™ºèƒ½ä½“åè°ƒæµ‹è¯•å¤±è´¥: {e}")

print("\n=== æ‰€æœ‰ç®€å•RLç»„ä»¶æµ‹è¯•å®Œæˆ ===")
print("ğŸ‰ RLæ ¸å¿ƒåŠŸèƒ½éªŒè¯é€šè¿‡ï¼æ¨¡å—æ¶æ„è®¾è®¡æ­£ç¡®ã€‚")
print("\nğŸ“ æ€»ç»“:")
print("- âœ… åŸºç¡€æ•°æ®ç»“æ„è®¾è®¡åˆç†")
print("- âœ… ç­–ç•¥ç½‘ç»œæ¶æ„å¯è¡Œ")
print("- âœ… ç»éªŒç®¡ç†æœºåˆ¶æœ‰æ•ˆ")
print("- âœ… å¥–åŠ±è®¡ç®—é€»è¾‘æ­£ç¡®")
print("- âœ… çŠ¶æ€ç¼–ç æ–¹æ¡ˆå¯è¡Œ")
print("- âœ… ç­–ç•¥æ›´æ–°æµç¨‹æ­£å¸¸")
print("- âœ… å¤šæ™ºèƒ½ä½“åè°ƒæœºåˆ¶æœ‰æ•ˆ")
print("\nğŸš€ RLå¢å¼ºå­¦ä¹ æ¨¡å—å·²å‡†å¤‡å°±ç»ªï¼")