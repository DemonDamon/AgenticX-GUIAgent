#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ActionReflectorAgent - å¤šæ¨¡æ€åŠ¨ä½œåæ€å™¨æ™ºèƒ½ä½“

åŸºäºAgenticXæ¡†æ¶å’ŒMobile Agent v3è®¾è®¡ç²¾é«“ï¼Œå®ç°çœŸæ­£çš„å¤šæ¨¡æ€LLMé©±åŠ¨çš„åŠ¨ä½œåæ€åˆ†æã€‚
è´Ÿè´£ï¼š
1. å¤šæ¨¡æ€åˆ†ææ‰§è¡Œå‰åçš„å±å¹•çŠ¶æ€å˜åŒ–
2. åŸºäºè§†è§‰ç†è§£åˆ¤æ–­æ“ä½œæˆåŠŸæ€§
3. ç”Ÿæˆæ™ºèƒ½åŒ–çš„æ”¹è¿›å»ºè®®å’Œå­¦ä¹ æ´å¯Ÿ
4. æ”¯æŒå¤šæ¨¡å‹é™çº§ç­–ç•¥ç¡®ä¿å¯é æ€§
"""

import asyncio
import copy
import json
from rich import print
from rich.json import JSON
from loguru import logger
import base64
import os
from typing import Dict, Any, List, Optional, Tuple
import json
from collections import defaultdict

# ä½¿ç”¨AgenticXæ ¸å¿ƒç»„ä»¶
from agenticx.core.agent import Agent, AgentResult
from agenticx.core.tool import BaseTool
from agenticx.core.event import Event, TaskStartEvent, TaskEndEvent, ReplanningRequiredEvent, ActionCorrectionEvent
from agenticx.core.event_bus import EventBus
from agenticx.llms.base import BaseLLMProvider
from agenticx.memory.component import MemoryComponent

from core.base_agent import BaseAgenticXGUIAgentAgent
from config import AgentConfig
from utils import get_iso_timestamp


class MultimodalActionAnalysisTool(BaseTool):
    """å¤šæ¨¡æ€åŠ¨ä½œåˆ†æå·¥å…· - åŸºäºMobile Agent v3çš„ActionReflectorè®¾è®¡ç²¾é«“"""
    
    name: str = "multimodal_action_analysis"
    description: str = "ä½¿ç”¨å¤šæ¨¡æ€LLMåˆ†ææ‰§è¡Œå‰åçš„å±å¹•çŠ¶æ€å˜åŒ–ï¼Œåˆ¤æ–­æ“ä½œæˆåŠŸæ€§"
    
    # æ·»åŠ è‡ªå®šä¹‰å±æ€§
    llm_provider: Optional[BaseLLMProvider] = None
    model_fallback_chain: List[Dict[str, str]] = []
    
    def __init__(self, llm_provider: Optional[BaseLLMProvider] = None, **kwargs):
        super().__init__(**kwargs)
        self.llm_provider = llm_provider
        
        # å®šä¹‰æ¨¡å‹é™çº§ç­–ç•¥
        self.model_fallback_chain = [
            {"provider": "bailian", "model": "qwen-vl-max"},
            {"provider": "bailian", "model": "qwen-vl-plus"},
            {"provider": "kimi", "model": "moonshot-v1-8k"}
        ]
    
    def execute(self, **kwargs) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œæ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        # ä»kwargsä¸­æå–action_dataå‚æ•°
        action_data = kwargs.get('action_data', {})
        
        if not self.llm_provider:
            logger.error("æœªé…ç½®LLMæä¾›è€…ï¼Œæ— æ³•æ‰§è¡Œå¤šæ¨¡æ€åˆ†æ")
            return {"success": False, "error": "æœªé…ç½®LLMæä¾›è€…"}
        
        try:
            # å°è¯•åŒæ­¥è°ƒç”¨LLMï¼ˆå¦‚æœæ”¯æŒï¼‰
            return self._sync_multimodal_analysis(action_data)
        except Exception as e:
            logger.error(f"åŒæ­¥å¤šæ¨¡æ€åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"åŒæ­¥åˆ†æå¤±è´¥: {str(e)}",
                "analysis_time": get_iso_timestamp(),
                "note": "å»ºè®®ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬aexecuteä»¥è·å¾—å¤šæ¨¡å‹é™çº§æ”¯æŒ"
            }
    
    def _extract_coordinate_feedback_from_analysis(self, coordinate_analysis: str, 
                                                 comparison_analysis: str, 
                                                 improvement_suggestions: str) -> Optional[Dict[str, Any]]:
        """ä»åˆ†ææ–‡æœ¬ä¸­æå–åæ ‡åé¦ˆä¿¡æ¯ - MultimodalActionAnalysisToolç‰ˆæœ¬"""
        try:
            # åˆå¹¶æ‰€æœ‰ç›¸å…³æ–‡æœ¬
            combined_text = f"{coordinate_analysis} {comparison_analysis} {improvement_suggestions}"
            
            if not combined_text.strip():
                return None
            
            # å°è¯•æå–ç²¾ç¡®çš„åƒç´ è°ƒæ•´
            import re
            
            adjustment_x = 0
            adjustment_y = 0
            reasons = []
            
            # æ°´å¹³æ–¹å‘è°ƒæ•´
            horizontal_patterns = [
                r'å‘å·¦è°ƒæ•´(\d+)åƒç´ ',
                r'å‘å³è°ƒæ•´(\d+)åƒç´ ',
                r'å·¦ç§»(\d+)åƒç´ ',
                r'å³ç§»(\d+)åƒç´ '
            ]
            
            for pattern in horizontal_patterns:
                matches = re.findall(pattern, combined_text)
                for match in matches:
                    pixels = int(match)
                    if 'å·¦' in pattern:
                        adjustment_x -= pixels
                        reasons.append(f"å‘å·¦è°ƒæ•´{pixels}åƒç´ ")
                    else:
                        adjustment_x += pixels
                        reasons.append(f"å‘å³è°ƒæ•´{pixels}åƒç´ ")
            
            # å‚ç›´æ–¹å‘è°ƒæ•´
            vertical_patterns = [
                r'å‘ä¸Šè°ƒæ•´(\d+)åƒç´ ',
                r'å‘ä¸‹è°ƒæ•´(\d+)åƒç´ ',
                r'ä¸Šç§»(\d+)åƒç´ ',
                r'ä¸‹ç§»(\d+)åƒç´ '
            ]
            
            for pattern in vertical_patterns:
                matches = re.findall(pattern, combined_text)
                for match in matches:
                    pixels = int(match)
                    if 'ä¸Š' in pattern:
                        adjustment_y -= pixels
                        reasons.append(f"å‘ä¸Šè°ƒæ•´{pixels}åƒç´ ")
                    else:
                        adjustment_y += pixels
                        reasons.append(f"å‘ä¸‹è°ƒæ•´{pixels}åƒç´ ")
            
            if adjustment_x != 0 or adjustment_y != 0:
                return {
                    "original_coordinates": [0, 0],  # å ä½ç¬¦
                    "suggested_adjustment": [adjustment_x, adjustment_y],
                    "reason": "åŸºäºå¤šæ¨¡æ€åˆ†æçš„åæ ‡è°ƒæ•´: " + ", ".join(set(reasons)),
                    "confidence": 0.85,
                    "analysis_method": "multimodal_tool_extraction"
                }
            
            return None
            
        except Exception as e:
              logger.error(f"MultimodalActionAnalysisToolåæ ‡åé¦ˆæå–å¤±è´¥: {e}")
              return None
    
    async def aexecute(self, **kwargs) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰§è¡Œå¤šæ¨¡æ€åŠ¨ä½œåˆ†æ - æ”¯æŒå¤šæ¨¡å‹é™çº§ç­–ç•¥
        
        Args:
            action_data: åŠ¨ä½œæ•°æ®ï¼ŒåŒ…å«:
                - before_screenshot: æ“ä½œå‰æˆªå›¾è·¯å¾„
                - after_screenshot: æ“ä½œåæˆªå›¾è·¯å¾„
                - action: æ‰§è¡Œçš„åŠ¨ä½œä¿¡æ¯
                - expectation: æœŸæœ›çš„ç»“æœ
            **kwargs: é¢å¤–å‚æ•°
        
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«æˆåŠŸåˆ¤æ–­ã€é”™è¯¯åˆ†æã€æ”¹è¿›å»ºè®®ç­‰
        """
        # ä»kwargsä¸­æå–action_dataå‚æ•°
        action_data = kwargs.get('action_data', {})
        
        if not self.llm_provider:
            logger.error("æœªé…ç½®LLMæä¾›è€…ï¼Œæ— æ³•æ‰§è¡Œå¤šæ¨¡æ€åˆ†æ")
            return {"success": False, "error": "æœªé…ç½®LLMæä¾›è€…"}
        
        # å°è¯•å¤šæ¨¡å‹é™çº§ç­–ç•¥
        for i, model_config in enumerate(self.model_fallback_chain):
            model_name = f"{model_config['provider']}/{model_config['model']}"
            try:
                logger.info(f"ğŸ¤– å°è¯•ä½¿ç”¨ {model_name} è¿›è¡ŒåŠ¨ä½œåæ€åˆ†æ...")
                
                # åˆ›å»ºå¯¹åº”çš„LLMæä¾›è€…
                provider = await self._create_provider(model_config)
                if not provider:
                    continue
                
                # æ‰§è¡Œå¤šæ¨¡æ€åˆ†æ
                result = await self._multimodal_reflection_analysis(
                    provider, action_data, model_config
                )
                
                logger.info(f"âœ… {model_name} åŠ¨ä½œåæ€åˆ†ææˆåŠŸ")
                return result
                
            except Exception as e:
                logger.warning(f"âŒ {model_name} åˆ†æå¤±è´¥: {e}")
                if i == len(self.model_fallback_chain) - 1:
                    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
                    logger.error("ğŸš¨ æ‰€æœ‰LLMæ¨¡å‹éƒ½å¤±è´¥ï¼ŒåŠ¨ä½œåæ€åˆ†ææ— æ³•å®Œæˆ")
                    return {
                        "success": False,
                        "error": f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥: {str(e)}",
                        "attempted_models": [f"{m['provider']}/{m['model']}" for m in self.model_fallback_chain],
                        "analysis_time": get_iso_timestamp()
                    }
                else:
                    next_model = self.model_fallback_chain[i+1]
                    next_model_name = f"{next_model['provider']}/{next_model['model']}"
                    logger.info(f"ğŸ”„ é™çº§åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {next_model_name}")
                    continue
        
        return {"success": False, "error": "æœªçŸ¥é”™è¯¯"}
    
    async def _create_provider(self, model_config: Dict[str, str]):
        """æ ¹æ®é…ç½®åˆ›å»ºLLMæä¾›è€…"""
        try:
            import os
            
            if model_config["provider"] == "bailian":
                from agenticx.llms.bailian_provider import BailianProvider
                api_key = os.getenv('BAILIAN_API_KEY')
                if not api_key:
                    model_name = f"{model_config['provider']}/{model_config['model']}"
                    logger.warning(f"æœªè®¾ç½®BAILIAN_API_KEYï¼Œè·³è¿‡{model_name}")
                    return None
                
                return BailianProvider(
                    api_key=api_key,
                    model=model_config["model"],
                    temperature=0.3,
                    timeout=60.0
                )
            
            elif model_config["provider"] == "kimi":
                from agenticx.llms.kimi_provider import KimiProvider
                api_key = os.getenv('MOONSHOT_API_KEY') or os.getenv('KIMI_API_KEY')
                if not api_key:
                    model_name = f"{model_config['provider']}/{model_config['model']}"
                    logger.warning(f"æœªè®¾ç½®MOONSHOT_API_KEYæˆ–KIMI_API_KEYï¼Œè·³è¿‡{model_name}")
                    return None
                
                return KimiProvider(
                    api_key=api_key,
                    model=model_config["model"],
                    temperature=0.3,
                    timeout=60.0
                )
            
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æä¾›è€…: {model_config['provider']}")
                return None
                
        except Exception as e:
            model_name = f"{model_config['provider']}/{model_config['model']}"
            logger.error(f"åˆ›å»º{model_name}æä¾›è€…å¤±è´¥: {e}")
            return None
    
    async def _multimodal_reflection_analysis(
        self, 
        provider, 
        action_data: Dict[str, Any], 
        model_config: Dict[str, str]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šæä¾›è€…æ‰§è¡Œå¤šæ¨¡æ€åæ€åˆ†æ - å‚è€ƒMobile Agent v3çš„ActionReflectorè®¾è®¡"""
        
        # æ„å»ºå¤šæ¨¡æ€åæ€æç¤ºè¯
        prompt = self._build_reflection_prompt(action_data)
        
        # æ·»åŠ æ“ä½œå‰åçš„æˆªå›¾è¿›è¡Œå¯¹æ¯”åˆ†æ
        before_screenshot = action_data.get("before_screenshot")
        after_screenshot = action_data.get("after_screenshot")
        
        # å‡†å¤‡å›¾åƒå†…å®¹
        if before_screenshot and after_screenshot:
            try:
                # è¯»å–æ“ä½œå‰æˆªå›¾
                with open(before_screenshot, "rb") as f:
                    before_image_base64 = base64.b64encode(f.read()).decode('utf-8')
                
                # è¯»å–æ“ä½œåæˆªå›¾
                with open(after_screenshot, "rb") as f:
                    after_image_base64 = base64.b64encode(f.read()).decode('utf-8')
                
                # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
                messages = [{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{before_image_base64}"}},
                        {"type": "text", "text": "\n\n### æ“ä½œåæˆªå›¾ ###"},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{after_image_base64}"}}
                    ]
                }]
            except Exception as e:
                logger.warning(f"è¯»å–æˆªå›¾æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å¼")
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼
                messages = [{"role": "user", "content": prompt}]
        else:
            # ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å¼
            messages = [{"role": "user", "content": prompt}]
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        # ä¸ºäº†æ—¥å¿—è®°å½•ï¼Œæˆªæ–­base64å­—ç¬¦ä¸²
        log_messages = copy.deepcopy(messages)
        for message in log_messages:
            if isinstance(message.get("content"), list):
                for item in message["content"]:
                    if item.get("type") == "image_url" and item.get("image_url", {}).get("url", "").startswith("data:image"):
                        item["image_url"]["url"] = item["image_url"]["url"][:50] + "..."
        logger.info(f"å‘é€ç»™reflectorçš„æŒ‡ä»¤: {log_messages}")
        response = await provider.ainvoke(messages)
        result = self._parse_reflection_response(response.content, action_data)
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
        result["model_used"] = f"{model_config['provider']}/{model_config['model']}"
        result["provider"] = model_config["provider"]
        
        return result
    
    def _build_reflection_prompt(self, action_data: Dict[str, Any]) -> str:
        """æ„å»ºåæ€åˆ†ææç¤ºè¯ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒåæ ‡ç²¾åº¦åˆ†æ"""
        
        action_info = action_data.get("action", {})
        expectation = action_data.get("expectation", "")
        
        prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§»åŠ¨è®¾å¤‡æ“ä½œåˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿé€šè¿‡å¯¹æ¯”æ“ä½œå‰åçš„æˆªå›¾æ¥åˆ¤æ–­æ“ä½œæ˜¯å¦æˆåŠŸï¼Œå¹¶æä¾›ç²¾ç¡®çš„åæ ‡ä¼˜åŒ–å»ºè®®ã€‚\n\n"
        
        prompt += "### ä»»åŠ¡ä¿¡æ¯ ###\n"
        prompt += f"æ‰§è¡Œçš„æ“ä½œï¼š{action_info}\n"
        prompt += f"æœŸæœ›ç»“æœï¼š{expectation}\n\n"
        
        # æ·»åŠ åæ ‡ä¿¡æ¯
        if "coordinates" in action_info:
            coords = action_info["coordinates"]
            prompt += f"æ‰§è¡Œåæ ‡ï¼š({coords.get('x', 0)}, {coords.get('y', 0)})\n\n"
        
        prompt += "### åˆ†æè¦æ±‚ ###\n"
        prompt += "è¯·ä»”ç»†å¯¹æ¯”æ“ä½œå‰åçš„ä¸¤å¼ æˆªå›¾ï¼Œç‰¹åˆ«æ³¨æ„ï¼š\n"
        prompt += "1. æ“ä½œæ˜¯å¦è¾¾åˆ°äº†é¢„æœŸæ•ˆæœ\n"
        prompt += "2. å¦‚æœæˆªå›¾ä¸­æœ‰ç´«è‰²ç‚¹æ ‡æ³¨ï¼Œåˆ†æç‚¹å‡»ä½ç½®æ˜¯å¦ç²¾ç¡®\n"
        prompt += "3. ç‚¹å‡»ä½ç½®ä¸ç›®æ ‡å…ƒç´ ä¸­å¿ƒçš„åç§»æƒ…å†µ\n"
        prompt += "4. æä¾›å…·ä½“çš„åƒç´ çº§åæ ‡è°ƒæ•´å»ºè®®\n\n"
        
        prompt += "### åˆ¤æ–­æ ‡å‡† ###\n"
        prompt += "A: æˆåŠŸæˆ–éƒ¨åˆ†æˆåŠŸ - æ“ä½œç»“æœç¬¦åˆé¢„æœŸ\n"
        prompt += "B: å¤±è´¥ - æ“ä½œå¯¼è‡´é”™è¯¯é¡µé¢æˆ–æ„å¤–ç»“æœ\n"
        prompt += "C: å¤±è´¥ - æ“ä½œæ²¡æœ‰äº§ç”Ÿä»»ä½•å˜åŒ–\n\n"
        
        prompt += "### ç‰¹åˆ«æ³¨æ„ ###\n"
        prompt += "å¯¹äºæ»‘åŠ¨æ“ä½œï¼šå¦‚æœæ“ä½œå‰åå†…å®¹å®Œå…¨ç›¸åŒï¼Œåˆ™è®¤ä¸ºæ˜¯Cç±»å¤±è´¥ï¼ˆå¯èƒ½å·²æ»‘åŠ¨åˆ°åº•éƒ¨ï¼‰\n"
        prompt += "å¯¹äºç‚¹å‡»æ“ä½œï¼š\n"
        prompt += "- æ£€æŸ¥æ˜¯å¦æ‰“å¼€äº†æ–°é¡µé¢ã€å¼¹å‡ºäº†èœå•æˆ–äº§ç”Ÿäº†é¢„æœŸçš„ç•Œé¢å˜åŒ–\n"
        prompt += "- å¦‚æœæœ‰ç´«è‰²ç‚¹æ ‡æ³¨ï¼Œåˆ†æç‚¹å‡»ä½ç½®æ˜¯å¦åœ¨ç›®æ ‡å…ƒç´ çš„æœ‰æ•ˆåŒºåŸŸå†…\n"
        prompt += "- è¯„ä¼°ç‚¹å‡»ä½ç½®ä¸ç›®æ ‡å…ƒç´ ä¸­å¿ƒçš„åç§»è·ç¦»\n"
        prompt += "å¯¹äºè¾“å…¥æ“ä½œï¼šæ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ­£ç¡®è¾“å…¥åˆ°ç›®æ ‡ä½ç½®\n\n"
        
        prompt += "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›åˆ†æç»“æœï¼š\n"
        prompt += "### å¯¹æ¯”åˆ†æ ###\n"
        prompt += "è¯¦ç»†æè¿°æ“ä½œå‰åæˆªå›¾çš„å·®å¼‚å’Œå˜åŒ–\n\n"
        
        prompt += "### æˆåŠŸåˆ¤æ–­ ###\n"
        prompt += "é€‰æ‹©Aã€Bæˆ–Cï¼Œå¹¶è¯´æ˜åˆ¤æ–­ç†ç”±\n\n"
        
        prompt += "### åæ ‡ç²¾åº¦åˆ†æ ###\n"
        prompt += "å¦‚æœæœ‰ç´«è‰²ç‚¹æ ‡æ³¨ï¼Œåˆ†æï¼š\n"
        prompt += "- ç‚¹å‡»ä½ç½®æ˜¯å¦å‡†ç¡®å‘½ä¸­ç›®æ ‡å…ƒç´ \n"
        prompt += "- ä¸ç›®æ ‡å…ƒç´ ä¸­å¿ƒçš„åç§»æ–¹å‘å’Œè·ç¦»ï¼ˆåƒç´ ï¼‰\n"
        prompt += "- å…·ä½“çš„åæ ‡è°ƒæ•´å»ºè®®ï¼ˆå¦‚ï¼šå‘å³è°ƒæ•´10åƒç´ ï¼Œå‘ä¸‹è°ƒæ•´5åƒç´ ï¼‰\n\n"
        
        prompt += "### é”™è¯¯åˆ†æ ###\n"
        prompt += "å¦‚æœæ“ä½œå¤±è´¥ï¼Œåˆ†æå¯èƒ½çš„åŸå› å’Œé”™è¯¯ç±»å‹\n\n"
        
        prompt += "### æ”¹è¿›å»ºè®® ###\n"
        prompt += "æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š\n"
        prompt += "- ç²¾ç¡®çš„åæ ‡è°ƒæ•´æ•°å€¼\n"
        prompt += "- æ“ä½œæ—¶æœºä¼˜åŒ–\n"
        prompt += "- å…¶ä»–æ‰§è¡Œç­–ç•¥å»ºè®®\n"
        
        return prompt
    
    def _parse_reflection_response(self, response_content: str, action_data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æåæ€åˆ†æå“åº” - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒåæ ‡ç²¾åº¦åˆ†æ"""
        try:
            import re
            
            # æå–å¯¹æ¯”åˆ†æ
            comparison_match = re.search(r'### å¯¹æ¯”åˆ†æ ###\s*(.+?)\s*### æˆåŠŸåˆ¤æ–­ ###', response_content, re.DOTALL)
            comparison_analysis = comparison_match.group(1).strip() if comparison_match else ""
            
            # æå–æˆåŠŸåˆ¤æ–­
            judgment_match = re.search(r'### æˆåŠŸåˆ¤æ–­ ###\s*(.+?)\s*### (åæ ‡ç²¾åº¦åˆ†æ|é”™è¯¯åˆ†æ) ###', response_content, re.DOTALL)
            success_judgment = judgment_match.group(1).strip() if judgment_match else ""
            
            # æå–åæ ‡ç²¾åº¦åˆ†æï¼ˆæ–°å¢ï¼‰
            coordinate_match = re.search(r'### åæ ‡ç²¾åº¦åˆ†æ ###\s*(.+?)\s*### é”™è¯¯åˆ†æ ###', response_content, re.DOTALL)
            coordinate_analysis = coordinate_match.group(1).strip() if coordinate_match else ""
            
            # æå–é”™è¯¯åˆ†æ
            error_match = re.search(r'### é”™è¯¯åˆ†æ ###\s*(.+?)\s*### æ”¹è¿›å»ºè®® ###', response_content, re.DOTALL)
            error_analysis = error_match.group(1).strip() if error_match else ""
            
            # æå–æ”¹è¿›å»ºè®®
            improvement_match = re.search(r'### æ”¹è¿›å»ºè®® ###\s*(.+?)$', response_content, re.DOTALL)
            improvement_suggestions = improvement_match.group(1).strip() if improvement_match else ""
            
            # åˆ¤æ–­æ“ä½œç»“æœ
            outcome = "A"  # é»˜è®¤æˆåŠŸ
            if "B" in success_judgment.upper():
                outcome = "B"
            elif "C" in success_judgment.upper():
                outcome = "C"
            
            success = outcome == "A"
            
            # æå–åæ ‡è°ƒæ•´ä¿¡æ¯
            coordinate_feedback = self._extract_coordinate_feedback_from_analysis(
                coordinate_analysis, comparison_analysis, improvement_suggestions
            )
            
            result = {
                "success": True,  # åˆ†ææˆåŠŸ
                "operation_success": success,  # æ“ä½œæ˜¯å¦æˆåŠŸ
                "outcome": outcome,
                "comparison_analysis": comparison_analysis,
                "success_judgment": success_judgment,
                "coordinate_analysis": coordinate_analysis,  # æ–°å¢åæ ‡åˆ†æ
                "error_analysis": error_analysis,
                "improvement_suggestions": improvement_suggestions,
                "coordinate_feedback": coordinate_feedback,  # æ–°å¢åæ ‡åé¦ˆ
                "full_response": response_content,
                "analysis_time": get_iso_timestamp(),
                "method": "enhanced_multimodal_llm_reflection"
            }
            
            # è®°å½•åæ ‡åˆ†æç»“æœ
            if coordinate_feedback:
                logger.info(f"ğŸ¯ æå–åˆ°åæ ‡åé¦ˆ: {coordinate_feedback}")
            
            return result
            
        except Exception as e:
            logger.error(f"è§£æåæ€å“åº”å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"è§£æå“åº”å¤±è´¥: {str(e)}",
                "full_response": response_content,
                "analysis_time": get_iso_timestamp()
            }
    
    def _sync_multimodal_analysis(self, action_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥å¤šæ¨¡æ€åˆ†æï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç®€åŒ–çš„åŒæ­¥å®ç°ï¼Œæä¾›åŸºæœ¬çš„åˆ†æåŠŸèƒ½
        try:
            # åŸºæœ¬çš„æˆåŠŸæ€§åˆ¤æ–­
            action_info = action_data.get("action", {})
            task_type = action_data.get("task_type", "unknown")
            
            # ç®€å•çš„å¯å‘å¼åˆ†æ
            success_score = 0.7  # é»˜è®¤æˆåŠŸåˆ†æ•°
            quality_score = 0.6  # é»˜è®¤è´¨é‡åˆ†æ•°
            
            return {
                "success": True,
                "task_type": task_type,
                "success_score": success_score,
                "quality_score": quality_score,
                "analysis_method": "sync_heuristic",
                "analysis_time": get_iso_timestamp(),
                "note": "åŒæ­¥ç®€åŒ–ç‰ˆæœ¬ï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬aexecuteä»¥è·å¾—å®Œæ•´çš„å¤šæ¨¡æ€LLMæ”¯æŒ"
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"åŒæ­¥åˆ†æå¤±è´¥: {str(e)}",
                "analysis_time": get_iso_timestamp()
            }
    
    def _calculate_efficiency_score(self, action_data: Dict[str, Any]) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•°"""
        result = action_data.get("result", {})
        duration = result.get("duration", 1.0)
        retry_count = action_data.get("retry_count", 0)
        
        # åŸºç¡€åˆ†æ•°
        base_score = 1.0
        
        # æ ¹æ®æ‰§è¡Œæ—¶é—´è°ƒæ•´
        if duration > 5.0:
            base_score *= 0.7
        elif duration > 2.0:
            base_score *= 0.9
        
        # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´
        base_score *= (1.0 - retry_count * 0.2)
        
        return max(0.0, min(1.0, base_score))
    
    def _calculate_accuracy_score(self, action_data: Dict[str, Any]) -> float:
        """è®¡ç®—å‡†ç¡®æ€§åˆ†æ•°"""
        success = action_data.get("success", False)
        result = action_data.get("result", {})
        
        if not success:
            return 0.0
        
        # æ ¹æ®ç»“æœä¸­çš„ç½®ä¿¡åº¦è°ƒæ•´
        confidence = result.get("confidence", 0.9)
        return confidence
    
    def _analyze_errors(self, action_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯"""
        error_info = action_data.get("error")
        if not error_info:
            return {"has_error": False}
        
        return {
            "has_error": True,
            "error_type": self._classify_error(error_info),
            "error_message": str(error_info),
            "potential_causes": self._identify_error_causes(error_info),
            "recovery_suggestions": self._suggest_error_recovery(error_info)
        }
    
    def _classify_error(self, error_info: str) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_str = str(error_info).lower()
        
        if "timeout" in error_str:
            return "timeout_error"
        elif "element" in error_str and "not found" in error_str:
            return "element_not_found"
        elif "permission" in error_str:
            return "permission_error"
        elif "network" in error_str or "connection" in error_str:
            return "network_error"
        else:
            return "unknown_error"
    
    def _identify_error_causes(self, error_info: str) -> List[str]:
        """è¯†åˆ«é”™è¯¯åŸå› """
        error_type = self._classify_error(error_info)
        
        causes_map = {
            "timeout_error": ["ç½‘ç»œå»¶è¿Ÿ", "è®¾å¤‡å“åº”æ…¢", "æ“ä½œå¤æ‚åº¦é«˜"],
            "element_not_found": ["UIç•Œé¢å˜åŒ–", "å…ƒç´ å®šä½ä¸å‡†ç¡®", "é¡µé¢åŠ è½½æœªå®Œæˆ"],
            "permission_error": ["æƒé™ä¸è¶³", "å®‰å…¨ç­–ç•¥é™åˆ¶", "åº”ç”¨çŠ¶æ€å¼‚å¸¸"],
            "network_error": ["ç½‘ç»œè¿æ¥é—®é¢˜", "æœåŠ¡å™¨ä¸å¯ç”¨", "ä»£ç†è®¾ç½®é—®é¢˜"],
            "unknown_error": ["æœªçŸ¥ç³»ç»Ÿé—®é¢˜", "ä»£ç é€»è¾‘é”™è¯¯", "ç¯å¢ƒé…ç½®é—®é¢˜"]
        }
        
        return causes_map.get(error_type, ["æœªçŸ¥åŸå› "])
    
    def _suggest_error_recovery(self, error_info: str) -> List[str]:
        """å»ºè®®é”™è¯¯æ¢å¤æ–¹æ¡ˆ"""
        error_type = self._classify_error(error_info)
        
        recovery_map = {
            "timeout_error": ["å¢åŠ è¶…æ—¶æ—¶é—´", "åˆ†è§£å¤æ‚æ“ä½œ", "æ£€æŸ¥ç½‘ç»œçŠ¶æ€"],
            "element_not_found": ["æ›´æ–°å…ƒç´ å®šä½ç­–ç•¥", "ç­‰å¾…é¡µé¢åŠ è½½", "ä½¿ç”¨å¤‡ç”¨å®šä½æ–¹æ³•"],
            "permission_error": ["æ£€æŸ¥åº”ç”¨æƒé™", "é‡å¯åº”ç”¨", "ä½¿ç”¨ç®¡ç†å‘˜æƒé™"],
            "network_error": ["æ£€æŸ¥ç½‘ç»œè¿æ¥", "é‡è¯•æ“ä½œ", "ä½¿ç”¨ç¦»çº¿æ¨¡å¼"],
            "unknown_error": ["é‡å¯ç³»ç»Ÿ", "æ£€æŸ¥æ—¥å¿—", "è”ç³»æŠ€æœ¯æ”¯æŒ"]
        }
        
        return recovery_map.get(error_type, ["é‡è¯•æ“ä½œ"])
    
    def _generate_suggestions(self, action_data: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åŸºäºæ•ˆç‡åˆ†æ•°çš„å»ºè®®
        efficiency = self._calculate_efficiency_score(action_data)
        if efficiency < 0.7:
            suggestions.append("è€ƒè™‘ä¼˜åŒ–æ“ä½œæµç¨‹ä»¥æé«˜æ•ˆç‡")
        
        # åŸºäºå‡†ç¡®æ€§åˆ†æ•°çš„å»ºè®®
        accuracy = self._calculate_accuracy_score(action_data)
        if accuracy < 0.8:
            suggestions.append("æé«˜å…ƒç´ å®šä½çš„å‡†ç¡®æ€§")
        
        # åŸºäºé‡è¯•æ¬¡æ•°çš„å»ºè®®
        retry_count = action_data.get("retry_count", 0)
        if retry_count > 1:
            suggestions.append("åˆ†æé‡è¯•åŸå› ï¼Œæ”¹è¿›åˆæ¬¡æ‰§è¡ŒæˆåŠŸç‡")
        
        # åŸºäºä»»åŠ¡ç±»å‹çš„å»ºè®®
        task_type = action_data.get("task_type", "")
        if task_type == "click_action":
            suggestions.append("ç¡®ä¿ç‚¹å‡»ç›®æ ‡å…ƒç´ å¯è§ä¸”å¯äº¤äº’")
        elif task_type == "input_text":
            suggestions.append("éªŒè¯è¾“å…¥æ¡†çŠ¶æ€å’Œæ–‡æœ¬æ ¼å¼")
        elif task_type == "swipe_action":
            suggestions.append("è°ƒæ•´æ»‘åŠ¨é€Ÿåº¦å’Œè·ç¦»")
        
        return suggestions if suggestions else ["å½“å‰æ“ä½œè¡¨ç°è‰¯å¥½"]


class PerformanceAnalysisTool(BaseTool):
    """æ€§èƒ½åˆ†æå·¥å…·"""
    
    name: str = "performance_analysis"
    description: str = "åˆ†ææ•´ä½“æ€§èƒ½è¡¨ç°"
    
    def execute(self, **kwargs) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œæ–¹æ³•"""
        # ä»kwargsä¸­æå–action_historyå‚æ•°
        action_history = kwargs.get('action_history', [])
        
        # ç›´æ¥è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œé¿å…å¼‚æ­¥è°ƒç”¨é—®é¢˜
        return {
            "performance_id": f"perf_{get_iso_timestamp()}",
            "total_actions": len(action_history),
            "success_rate": 0.9,
            "avg_execution_time": 2.5,
            "bottlenecks": [],
            "improvements": ["ä¼˜åŒ–æ‰§è¡Œé€Ÿåº¦"],
            "success": True
        }
    
    async def aexecute(self, **kwargs) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½
        
        Args:
            action_history: åŠ¨ä½œå†å²
            **kwargs: é¢å¤–å‚æ•°
        
        Returns:
            æ€§èƒ½åˆ†æç»“æœ
        """
        # ä»kwargsä¸­æå–action_historyå‚æ•°
        action_history = kwargs.get('action_history', [])
        
        await asyncio.sleep(1.0)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
        
        if not action_history:
            return {
                "total_actions": 0,
                "success_rate": 0.0,
                "average_efficiency": 0.0,
                "performance_trends": [],
                "analysis_time": get_iso_timestamp()
            }
        
        # ç»Ÿè®¡åŸºæœ¬æŒ‡æ ‡
        total_actions = len(action_history)
        successful_actions = sum(1 for action in action_history if action.get("success", False))
        success_rate = successful_actions / total_actions if total_actions > 0 else 0.0
        
        # è®¡ç®—å¹³å‡æ•ˆç‡
        efficiency_scores = []
        for action in action_history:
            result = action.get("result", {})
            duration = result.get("duration", 1.0)
            retry_count = action.get("retry_count", 0)
            
            efficiency = 1.0 - (duration / 10.0) - (retry_count * 0.2)
            efficiency_scores.append(max(0.0, min(1.0, efficiency)))
        
        average_efficiency = sum(efficiency_scores) / len(efficiency_scores) if efficiency_scores else 0.0
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        performance_trends = self._analyze_trends(action_history)
        
        # è¯†åˆ«é—®é¢˜æ¨¡å¼
        problem_patterns = self._identify_problem_patterns(action_history)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(
            success_rate, average_efficiency, problem_patterns
        )
        
        return {
            "total_actions": total_actions,
            "successful_actions": successful_actions,
            "success_rate": success_rate,
            "average_efficiency": average_efficiency,
            "performance_trends": performance_trends,
            "problem_patterns": problem_patterns,
            "optimization_suggestions": optimization_suggestions,
            "analysis_time": get_iso_timestamp()
        }
    
    def _analyze_trends(self, action_history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(action_history) < 5:
            return []
        
        # æŒ‰æ—¶é—´çª—å£åˆ†æ
        window_size = max(5, len(action_history) // 4)
        trends = []
        
        for i in range(0, len(action_history) - window_size + 1, window_size):
            window = action_history[i:i + window_size]
            window_success_rate = sum(1 for action in window if action.get("success", False)) / len(window)
            
            trends.append({
                "window_start": i,
                "window_end": i + window_size - 1,
                "success_rate": window_success_rate,
                "action_count": len(window)
            })
        
        return trends
    
    def _identify_problem_patterns(self, action_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯†åˆ«é—®é¢˜æ¨¡å¼"""
        patterns = {
            "frequent_failures": [],
            "slow_operations": [],
            "retry_patterns": [],
            "error_clusters": []
        }
        
        # ç»Ÿè®¡å¤±è´¥é¢‘ç¹çš„æ“ä½œç±»å‹
        failure_counts = defaultdict(int)
        total_counts = defaultdict(int)
        
        for action in action_history:
            task_type = action.get("task_type", "unknown")
            total_counts[task_type] += 1
            if not action.get("success", False):
                failure_counts[task_type] += 1
        
        for task_type, failure_count in failure_counts.items():
            total_count = total_counts[task_type]
            failure_rate = failure_count / total_count
            if failure_rate > 0.3:  # å¤±è´¥ç‡è¶…è¿‡30%
                patterns["frequent_failures"].append({
                    "task_type": task_type,
                    "failure_rate": failure_rate,
                    "failure_count": failure_count,
                    "total_count": total_count
                })
        
        # è¯†åˆ«æ…¢æ“ä½œ
        for action in action_history:
            result = action.get("result", {})
            duration = result.get("duration", 0)
            if duration > 3.0:  # è¶…è¿‡3ç§’çš„æ“ä½œ
                patterns["slow_operations"].append({
                    "task_type": action.get("task_type", "unknown"),
                    "duration": duration,
                    "timestamp": action.get("timestamp")
                })
        
        # è¯†åˆ«é‡è¯•æ¨¡å¼
        for action in action_history:
            retry_count = action.get("retry_count", 0)
            if retry_count > 0:
                patterns["retry_patterns"].append({
                    "task_type": action.get("task_type", "unknown"),
                    "retry_count": retry_count,
                    "final_success": action.get("success", False)
                })
        
        return patterns
    
    def _generate_optimization_suggestions(self, success_rate: float, efficiency: float, patterns: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºæˆåŠŸç‡çš„å»ºè®®
        if success_rate < 0.8:
            suggestions.append("æ•´ä½“æˆåŠŸç‡åä½ï¼Œéœ€è¦æ”¹è¿›æ“ä½œç­–ç•¥")
        
        # åŸºäºæ•ˆç‡çš„å»ºè®®
        if efficiency < 0.7:
            suggestions.append("æ“ä½œæ•ˆç‡æœ‰å¾…æå‡ï¼Œè€ƒè™‘ä¼˜åŒ–æ‰§è¡Œæµç¨‹")
        
        # åŸºäºé—®é¢˜æ¨¡å¼çš„å»ºè®®
        if patterns["frequent_failures"]:
            suggestions.append("æŸäº›æ“ä½œç±»å‹å¤±è´¥ç‡è¾ƒé«˜ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–")
        
        if patterns["slow_operations"]:
            suggestions.append("å­˜åœ¨æ‰§è¡Œç¼“æ…¢çš„æ“ä½œï¼Œå»ºè®®ä¼˜åŒ–æˆ–åˆ†è§£")
        
        if patterns["retry_patterns"]:
            suggestions.append("é‡è¯•æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®æ”¹è¿›åˆæ¬¡æ‰§è¡ŒæˆåŠŸç‡")
        
        return suggestions if suggestions else ["æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ"]


class LearningInsightTool(BaseTool):
    """å­¦ä¹ æ´å¯Ÿå·¥å…·"""
    
    name: str = "learning_insight"
    description: str = "ç”Ÿæˆå­¦ä¹ æ´å¯Ÿå’ŒçŸ¥è¯†"
    
    def execute(self, **kwargs) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œæ–¹æ³•"""
        # ä»kwargsä¸­æå–analysis_resultså‚æ•°
        analysis_results = kwargs.get('analysis_results', [])
        
        # ç›´æ¥è¿”å›æ¨¡æ‹Ÿç»“æœï¼Œé¿å…å¼‚æ­¥è°ƒç”¨é—®é¢˜
        return {
            "insight_id": f"insight_{get_iso_timestamp()}",
            "patterns_identified": ["ç”¨æˆ·åå¥½æ¨¡å¼"],
            "learning_points": ["æé«˜å‡†ç¡®æ€§"],
            "strategy_updates": [],
            "confidence_score": 0.8,
            "success": True
        }
    
    async def aexecute(self, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆå­¦ä¹ æ´å¯Ÿ
        
        Args:
            analysis_results: åˆ†æç»“æœåˆ—è¡¨
            **kwargs: é¢å¤–å‚æ•°
        
        Returns:
            å­¦ä¹ æ´å¯Ÿ
        """
        # ä»kwargsä¸­æå–analysis_resultså‚æ•°
        analysis_results = kwargs.get('analysis_results', [])
        
        await asyncio.sleep(0.8)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
        
        insights = {
            "key_learnings": self._extract_key_learnings(analysis_results),
            "best_practices": self._identify_best_practices(analysis_results),
            "common_pitfalls": self._identify_common_pitfalls(analysis_results),
            "improvement_opportunities": self._identify_improvements(analysis_results),
            "knowledge_patterns": self._extract_knowledge_patterns(analysis_results),
            "insight_time": get_iso_timestamp()
        }
        
        return insights
    
    def _extract_key_learnings(self, analysis_results: List[Dict[str, Any]]) -> List[str]:
        """æå–å…³é”®å­¦ä¹ ç‚¹"""
        learnings = []
        
        # åˆ†ææˆåŠŸæ¨¡å¼
        successful_patterns = []
        for result in analysis_results:
            if result.get("success", False) and result.get("efficiency_score", 0) > 0.8:
                successful_patterns.append(result.get("task_type", "unknown"))
        
        if successful_patterns:
            most_successful = max(set(successful_patterns), key=successful_patterns.count)
            learnings.append(f"{most_successful}æ“ä½œè¡¨ç°æœ€ä½³ï¼Œå¯ä½œä¸ºæ ‡å‡†æ¨¡å¼")
        
        # åˆ†æå¤±è´¥æ¨¡å¼
        failed_patterns = []
        for result in analysis_results:
            if not result.get("success", False):
                failed_patterns.append(result.get("task_type", "unknown"))
        
        if failed_patterns:
            most_failed = max(set(failed_patterns), key=failed_patterns.count)
            learnings.append(f"{most_failed}æ“ä½œå¤±è´¥ç‡è¾ƒé«˜ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨")
        
        return learnings
    
    def _identify_best_practices(self, analysis_results: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«æœ€ä½³å®è·µ"""
        practices = []
        
        # åŸºäºé«˜æ•ˆç‡æ“ä½œçš„å®è·µ
        high_efficiency_actions = [r for r in analysis_results if r.get("efficiency_score", 0) > 0.9]
        if high_efficiency_actions:
            practices.append("ä¿æŒç®€æ´çš„æ“ä½œæµç¨‹ï¼Œé¿å…ä¸å¿…è¦çš„æ­¥éª¤")
        
        # åŸºäºé«˜å‡†ç¡®æ€§æ“ä½œçš„å®è·µ
        high_accuracy_actions = [r for r in analysis_results if r.get("accuracy_score", 0) > 0.95]
        if high_accuracy_actions:
            practices.append("ä½¿ç”¨ç²¾ç¡®çš„å…ƒç´ å®šä½ç­–ç•¥")
        
        # åŸºäºé›¶é‡è¯•æ“ä½œçš„å®è·µ
        zero_retry_actions = [r for r in analysis_results if r.get("retry_count", 0) == 0]
        if len(zero_retry_actions) > len(analysis_results) * 0.8:
            practices.append("å……åˆ†çš„é¢„æ£€æŸ¥å¯ä»¥å‡å°‘é‡è¯•æ¬¡æ•°")
        
        return practices
    
    def _identify_common_pitfalls(self, analysis_results: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«å¸¸è§é™·é˜±"""
        pitfalls = []
        
        # åˆ†æé”™è¯¯æ¨¡å¼
        error_types = []
        for result in analysis_results:
            error_analysis = result.get("error_analysis", {})
            if error_analysis.get("has_error"):
                error_types.append(error_analysis.get("error_type", "unknown"))
        
        if error_types:
            most_common_error = max(set(error_types), key=error_types.count)
            pitfalls.append(f"æœ€å¸¸è§çš„é”™è¯¯ç±»å‹æ˜¯{most_common_error}")
        
        # åˆ†æä½æ•ˆç‡æ¨¡å¼
        low_efficiency_count = sum(1 for r in analysis_results if r.get("efficiency_score", 1) < 0.5)
        if low_efficiency_count > len(analysis_results) * 0.3:
            pitfalls.append("æ“ä½œæ•ˆç‡æ™®éåä½ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é—®é¢˜")
        
        return pitfalls
    
    def _identify_improvements(self, analysis_results: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«æ”¹è¿›æœºä¼š"""
        improvements = []
        
        # æ”¶é›†æ‰€æœ‰æ”¹è¿›å»ºè®®
        all_suggestions = []
        for result in analysis_results:
            suggestions = result.get("improvement_suggestions", [])
            all_suggestions.extend(suggestions)
        
        # ç»Ÿè®¡æœ€é¢‘ç¹çš„å»ºè®®
        if all_suggestions:
            suggestion_counts = defaultdict(int)
            for suggestion in all_suggestions:
                suggestion_counts[suggestion] += 1
            
            # å–å‰3ä¸ªæœ€é¢‘ç¹çš„å»ºè®®
            top_suggestions = sorted(suggestion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            improvements = [suggestion for suggestion, count in top_suggestions]
        
        return improvements
    
    def _extract_knowledge_patterns(self, analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–çŸ¥è¯†æ¨¡å¼"""
        patterns = {
            "success_factors": [],
            "failure_factors": [],
            "efficiency_factors": [],
            "timing_patterns": []
        }
        
        # åˆ†ææˆåŠŸå› ç´ 
        successful_results = [r for r in analysis_results if r.get("success", False)]
        if successful_results:
            avg_efficiency = sum(r.get("efficiency_score", 0) for r in successful_results) / len(successful_results)
            patterns["success_factors"].append(f"æˆåŠŸæ“ä½œçš„å¹³å‡æ•ˆç‡: {avg_efficiency:.2f}")
        
        # åˆ†æå¤±è´¥å› ç´ 
        failed_results = [r for r in analysis_results if not r.get("success", False)]
        if failed_results:
            common_error_types = [r.get("error_analysis", {}).get("error_type") for r in failed_results]
            if common_error_types:
                most_common = max(set(common_error_types), key=common_error_types.count)
                patterns["failure_factors"].append(f"æœ€å¸¸è§å¤±è´¥åŸå› : {most_common}")
        
        return patterns


class ActionReflectorAgent(BaseAgenticXGUIAgentAgent):
    """å¤šæ¨¡æ€åŠ¨ä½œåæ€å™¨æ™ºèƒ½ä½“ - åŸºäºAgenticXæ¡†æ¶å’ŒMobile Agent v3è®¾è®¡ç²¾é«“
    
    è´Ÿè´£ï¼š
    1. å¤šæ¨¡æ€åˆ†ææ‰§è¡Œå‰åçš„å±å¹•çŠ¶æ€å˜åŒ–
    2. åŸºäºè§†è§‰ç†è§£åˆ¤æ–­æ“ä½œæˆåŠŸæ€§
    3. ç”Ÿæˆæ™ºèƒ½åŒ–çš„æ”¹è¿›å»ºè®®å’Œå­¦ä¹ æ´å¯Ÿ
    4. æ”¯æŒå¤šæ¨¡å‹é™çº§ç­–ç•¥ç¡®ä¿å¯é æ€§
    5. ä¸å…¶ä»–æ™ºèƒ½ä½“åä½œè¿›è¡ŒæŒç»­å­¦ä¹ 
    """
    
    def __init__(
        self,
        llm_provider: Optional[BaseLLMProvider] = None,
        agent_id: str = "action_reflector",
        platform = None,
        info_pool = None,
        learning_engine = None,
        agent_config: Optional[AgentConfig] = None,
        memory: Optional[MemoryComponent] = None
    ):
        # å­˜å‚¨é¢å¤–å‚æ•°
        self.agent_id = agent_id
        self.platform = platform
        self.info_pool = info_pool
        self.learning_engine = learning_engine
        
        # åˆ›å»ºé»˜è®¤é…ç½®ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if agent_config is None:
            agent_config = AgentConfig(
                id=agent_id,
                name="ActionReflectorAgent",
                role="action_reflector",
                goal="åŸºäºå¤šæ¨¡æ€LLMåˆ†ææ“ä½œå‰åçš„å±å¹•å˜åŒ–ï¼Œåˆ¤æ–­æ“ä½œæˆåŠŸæ€§å¹¶æä¾›æ”¹è¿›å»ºè®®",
                backstory="æˆ‘æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŠ¨ä½œåæ€æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿé€šè¿‡è§†è§‰ç†è§£åˆ†ææ“ä½œæ•ˆæœï¼Œå‚è€ƒMobile Agent v3çš„ActionReflectorè®¾è®¡ï¼ŒåŸºäºAgenticXæ¡†æ¶å®ç°ã€‚",
                tools=[]
            )
        
        # åˆå§‹åŒ–å¤šæ¨¡æ€å·¥å…·
        tools = [
            MultimodalActionAnalysisTool(llm_provider=llm_provider),
            # ä¿ç•™ä¸€äº›ä¼ ç»Ÿåˆ†æå·¥å…·ä½œä¸ºå¤‡ç”¨
            PerformanceAnalysisTool(),
            LearningInsightTool()
        ]
        
        super().__init__(agent_config, llm_provider, memory, tools, info_pool=info_pool)
        
        # åæ€åˆ†æçŠ¶æ€
        self.reflection_history: List[Dict[str, Any]] = []
        self.performance_metrics: Dict[str, Any] = {}
        self.learning_insights: Dict[str, Any] = {}
        self.screenshot_pairs: List[Tuple[str, str]] = []  # å­˜å‚¨æ“ä½œå‰åæˆªå›¾å¯¹
    
    async def _execute_task_impl(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œåæ€åˆ†æ - ActionReflectoræ ¸å¿ƒåŠŸèƒ½
        
        è¾“å…¥ï¼šExecutorçš„æ‰§è¡Œç»“æœ
        åŠŸèƒ½ï¼šæ‰§è¡Œè´¨é‡è¯„ä¼°ã€é—®é¢˜è¯†åˆ«ã€æ”¹è¿›å»ºè®®
        è¾“å‡ºï¼šåæ€åˆ†æå’Œæ”¹è¿›å»ºè®®
        
        Args:
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«:
                - task_id: ä»»åŠ¡ID
                - execution_result: Executorçš„æ‰§è¡Œç»“æœ
                - prompt: åæ€æç¤ºè¯
                - before_screenshot: æ“ä½œå‰æˆªå›¾è·¯å¾„
                - after_screenshot: æ“ä½œåæˆªå›¾è·¯å¾„
                - action_info: æ‰§è¡Œçš„åŠ¨ä½œä¿¡æ¯
                - expectation: æœŸæœ›ç»“æœ
        
        Returns:
            åæ€åˆ†æç»“æœ
        """
        task_id = task_context.get("task_id")
        execution_result = task_context.get("execution_result", {})
                
        # æ£€æŸ¥æ˜¯å¦æœ‰execution_resultï¼ˆæ¥è‡ªcollaboration.pyçš„è°ƒç”¨ï¼‰
        if execution_result:
            return await self._analyze_execution_result(task_context)
        
        # å¦åˆ™æŒ‰åŸæœ‰é€»è¾‘å¤„ç†ï¼ˆç›´æ¥çš„å¤šæ¨¡æ€åˆ†æè°ƒç”¨ï¼‰
        analysis_type = task_context.get("analysis_type", "multimodal_reflection")
        logger.info(f"ğŸ” æ‰§è¡Œå¤šæ¨¡æ€åæ€åˆ†æ: {analysis_type}")
        
        try:
            if analysis_type == "multimodal_reflection":
                result = await self._multimodal_action_reflection(task_context)
            elif analysis_type == "performance_analysis":
                result = await self._analyze_performance(task_context)
            elif analysis_type == "learning_insight":
                result = await self._generate_learning_insights(task_context)
            elif analysis_type == "comprehensive_analysis":
                result = await self._comprehensive_multimodal_analysis(task_context)
            else:
                # é»˜è®¤ä½¿ç”¨å¤šæ¨¡æ€åæ€åˆ†æ
                result = await self._multimodal_action_reflection(task_context)
            
            # è®°å½•åæ€å†å²
            reflection_record = {
                "analysis_type": analysis_type,
                "task_context": task_context,
                "result": result,
                "timestamp": get_iso_timestamp(),
                "model_used": result.get("model_used", "unknown")
            }
            self.reflection_history.append(reflection_record)
            
            # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
            if len(self.reflection_history) > 100:
                self.reflection_history = self.reflection_history[-100:]
            
            # å‘å¸ƒåæ€ç»“æœäº‹ä»¶
            reflection_event = Event(
                type="multimodal_reflection_result",
                data={
                    "agent_id": self.config.id,
                    "reflection_record": reflection_record
                },
                agent_id=self.config.id
            )
            await self.info_pool.publish_async(reflection_event)
            
            # å¦‚æœæ“ä½œå¤±è´¥ï¼Œå‘é€å…·ä½“çš„æ”¹è¿›å»ºè®®ç»™ExecutorAgent
            if not result.get("operation_success", True):
                await self._send_improvement_feedback_to_executor(result, task_context)
            
            logger.info(f"âœ… å¤šæ¨¡æ€åæ€åˆ†æå®Œæˆ: {analysis_type}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡æ€åæ€åˆ†æå¤±è´¥: {analysis_type}, é”™è¯¯: {e}")
            raise
    
    async def _multimodal_action_reflection(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """å¤šæ¨¡æ€åŠ¨ä½œåæ€åˆ†æ - æ ¸å¿ƒæ–¹æ³•
        
        Args:
            task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡
        
        Returns:
            åæ€åˆ†æç»“æœ
        """
        # å‡†å¤‡åˆ†ææ•°æ®
        action_info = task_context.get("action_info", {})
        action_data = {
            "before_screenshot": task_context.get("before_screenshot"),
            "after_screenshot": task_context.get("after_screenshot"),
            "action": action_info,
            "expectation": task_context.get("expectation", ""),
            "task_type": action_info.get("action", task_context.get("task_type", "unknown"))
        }
        
        # éªŒè¯å¿…è¦çš„è¾“å…¥
        if not action_data["before_screenshot"] or not action_data["after_screenshot"]:
            logger.warning("ç¼ºå°‘æ“ä½œå‰åæˆªå›¾ï¼Œæ— æ³•è¿›è¡Œå¤šæ¨¡æ€åˆ†æ")
            return {
                "success": False,
                "error": "ç¼ºå°‘å¿…è¦çš„æˆªå›¾æ–‡ä»¶",
                "analysis_time": get_iso_timestamp()
            }
        
        # æ£€æŸ¥æˆªå›¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(action_data["before_screenshot"]) or not os.path.exists(action_data["after_screenshot"]):
            logger.error("æˆªå›¾æ–‡ä»¶ä¸å­˜åœ¨")
            return {
                "success": False,
                "error": "æˆªå›¾æ–‡ä»¶ä¸å­˜åœ¨",
                "analysis_time": get_iso_timestamp()
            }
        
        # ä½¿ç”¨å¤šæ¨¡æ€åˆ†æå·¥å…·
        analysis_tool = self.get_tool("multimodal_action_analysis")
        if analysis_tool:
            result = analysis_tool.execute(action_data=action_data)
        else:
            result = {"success": False, "error": "æœªæ‰¾åˆ°åˆ†æå·¥å…·"}
        
        logger.info(f"å•ä¸ªåŠ¨ä½œåˆ†æå®Œæˆ: {action_data.get('task_type', 'unknown')}")
        return result
    
    async def _analyze_execution_result(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æExecutoræ‰§è¡Œç»“æœ - ActionReflectoræ ¸å¿ƒåŠŸèƒ½
        
        è¾“å…¥ï¼šExecutorçš„æ‰§è¡Œç»“æœ
        åŠŸèƒ½ï¼šæ‰§è¡Œè´¨é‡è¯„ä¼°ã€é—®é¢˜è¯†åˆ«ã€æ”¹è¿›å»ºè®®
        è¾“å‡ºï¼šåæ€åˆ†æå’Œæ”¹è¿›å»ºè®®
        """
        task_id = task_context.get("task_id")
        execution_result = task_context.get("execution_result", {})
        execution_details = execution_result.get("execution_details", {})
        before_screenshot = execution_details.get("before_screenshot")
        after_screenshot = execution_details.get("after_screenshot")
        action_info = execution_details.get("llm_action_plan", task_context.get("action_info", {}))
        expectation = task_context.get("expectation", "æ“ä½œæˆåŠŸæ‰§è¡Œ")
        
        logger.info(f"reflectoråˆ†æä»»åŠ¡ID: {task_id}")
        logger.info("ä¸Šä¸€æ­¥æ‰§è¡Œç»“æœ:"); print(execution_result)
        logger.info(f"åŠ¨ä½œæ‰§è¡Œã€Œå‰ã€çŠ¶æ€: {before_screenshot}")
        logger.info(f"åŠ¨ä½œæ‰§è¡Œã€Œåã€çŠ¶æ€: {after_screenshot}")
        logger.info("åŠ¨ä½œæ‰§è¡Œç»†èŠ‚:"); print(action_info)
        logger.info(f"é¢„æœŸç»“æœ: {expectation}")
        
        try:
            # 1. æ‰§è¡Œè´¨é‡è¯„ä¼°
            logger.info(f"ğŸ” æ­¥éª¤1: æ‰§è¡Œè´¨é‡è¯„ä¼°")
            quality_assessment = await self._assess_execution_quality(execution_result, action_info)
            
            # 2. é—®é¢˜è¯†åˆ«
            logger.info(f"ğŸ” æ­¥éª¤2: é—®é¢˜è¯†åˆ«")
            problem_identification = await self._identify_problems(execution_result, action_info)
            
            # 3. å¤šæ¨¡æ€åˆ†æï¼ˆå¦‚æœæœ‰æˆªå›¾ï¼‰
            multimodal_analysis = None
            if before_screenshot and after_screenshot:
                logger.info(f"ğŸ” æ­¥éª¤3: å¤šæ¨¡æ€åˆ†æ")
                multimodal_analysis = await self._perform_multimodal_analysis(
                    before_screenshot, after_screenshot, action_info, expectation
                )
            else:
                logger.warning(f"âš ï¸ è·³è¿‡å¤šæ¨¡æ€åˆ†æ: ç¼ºå°‘æˆªå›¾æ–‡ä»¶")
            
            # 4. æ”¹è¿›å»ºè®®ç”Ÿæˆ
            logger.info(f"ğŸ” æ­¥éª¤4: æ”¹è¿›å»ºè®®ç”Ÿæˆ")
            improvement_suggestions = await self._generate_improvement_suggestions(
                quality_assessment, problem_identification, multimodal_analysis
            )
            
            # 5. ç»¼åˆåæ€åˆ†æç»“æœ
            logger.info(f"ğŸ” æ­¥éª¤5: ç»¼åˆè¯„åˆ†è®¡ç®—")
            overall_score = self._calculate_overall_score(quality_assessment, problem_identification)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é€»è¾‘é—®é¢˜ï¼šæ‰§è¡ŒæˆåŠŸä½†å¤šæ¨¡æ€åˆ†ææ˜¾ç¤ºå¤±è´¥
            execution_success = execution_result.get("success", False)
            multimodal_success = multimodal_analysis.get("operation_success", True) if multimodal_analysis else True
            
            logger.info(f"æˆåŠŸæ€§å¯¹æ¯”æ£€æŸ¥:")
            logger.info(f"  - æ‰§è¡Œå™¨æŠ¥å‘ŠæˆåŠŸ: {execution_success}")
            logger.info(f"  - å¤šæ¨¡æ€åˆ†ææˆåŠŸ: {multimodal_success}")
            
            if execution_success and not multimodal_success:
                logger.warning(f"âš ï¸ å‘ç°é€»è¾‘çŸ›ç›¾: æ‰§è¡Œå™¨æŠ¥å‘ŠæˆåŠŸä½†å¤šæ¨¡æ€åˆ†ææ˜¾ç¤ºå¤±è´¥")
                # åº”è¯¥é™ä½æ€»ä½“è¯„åˆ†
                overall_score *= 0.3  # å¤§å¹…é™ä½è¯„åˆ†
                logger.info(f"çŸ›ç›¾æƒ©ç½šåè¯„åˆ†: {overall_score:.3f}")
            
            reflection_result = {
                "success": True,
                "task_id": task_id,
                "quality_assessment": quality_assessment,
                "problem_identification": problem_identification,
                "multimodal_analysis": multimodal_analysis,
                "improvement_suggestions": improvement_suggestions,
                "overall_score": overall_score,
                "reflection_time": get_iso_timestamp(),
                "analysis_method": "comprehensive_reflection",
                "execution_success": execution_success,
                "multimodal_success": multimodal_success,
                "has_logic_contradiction": execution_success and not multimodal_success
            }
            
            logger.info(f"âœ… æ‰§è¡Œç»“æœåˆ†æå®Œæˆï¼Œæ€»ä½“è¯„åˆ†: {reflection_result['overall_score']:.3f}")
            logger.info(f"æœ€ç»ˆåæ€ç»“æœæ‘˜è¦:")
            logger.info(f"  - è´¨é‡è¯„ä¼°: {quality_assessment.get('assessment')}")
            logger.info(f"  - é—®é¢˜æ•°é‡: {problem_identification.get('total_problems')}")
            logger.info(f"  - å¤šæ¨¡æ€åˆ†æ: {'æˆåŠŸ' if multimodal_analysis and multimodal_analysis.get('success') else 'å¤±è´¥/è·³è¿‡'}")
            logger.info(f"  - æ”¹è¿›å»ºè®®æ•°: {len(improvement_suggestions)}")

            # å†³å®šå¹¶å‘å¸ƒä¸‹ä¸€æ­¥äº‹ä»¶
            await self._decide_and_publish_next_step(reflection_result, task_context)
            
            return reflection_result
            
        except Exception as e:
            logger.error(f"âŒ æ‰§è¡Œç»“æœåˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "task_id": task_id,
                "error": str(e),
                "reflection_time": get_iso_timestamp()
            }

    async def _decide_and_publish_next_step(self, reflection_result: Dict[str, Any], task_context: Dict[str, Any]):
        """
        æ ¹æ®åæ€åˆ†æå†³å®šä¸‹ä¸€æ­¥ï¼Œå¹¶å‘å¸ƒç›¸åº”äº‹ä»¶ã€‚
        """
        execution_success = reflection_result.get("execution_success", False)
        multimodal_success = reflection_result.get("multimodal_success", True)
        improvement_suggestions = reflection_result.get("improvement_suggestions", [])
        task_id = task_context.get("task_id")

        if not execution_success or not multimodal_success:
            logger.info("ğŸ¤” æ“ä½œå¤±è´¥ï¼Œå†³ç­–ä¸‹ä¸€æ­¥...")

            # ä¼˜å…ˆå¤„ç†æœ‰æ˜ç¡®ä¿®æ­£å»ºè®®çš„æƒ…å†µ
            # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè¿™é‡Œçš„é€»è¾‘ä¼šæ›´å¤æ‚ï¼Œä¾‹å¦‚åˆ¤æ–­å»ºè®®ç±»å‹
            if improvement_suggestions and improvement_suggestions[0].get("type") == "correction":
                corrected_action = improvement_suggestions[0].get("action")
                reason = improvement_suggestions[0].get("reason")
                
                logger.info(f"âœ… å‘ç°å¯è¡Œçš„ä¿®æ­£å»ºè®®ï¼Œå‘å¸ƒ ActionCorrectionEvent")
                
                correction_event = ActionCorrectionEvent(
                    task_id=task_id,
                    original_action=task_context.get("action_info"),
                    corrected_action=corrected_action,
                    reason=reason,
                    reflection_result=reflection_result
                )
                await self.info_pool.publish_async(correction_event)

            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¿®æ­£å»ºè®®ï¼Œæˆ–è€…é—®é¢˜æ¯”è¾ƒä¸¥é‡ï¼Œåˆ™è¯·æ±‚é‡è§„åˆ’
                logger.warning("âŒ æœªæ‰¾åˆ°ç®€å•çš„ä¿®æ­£æ–¹æ³•ï¼Œå‘å¸ƒ ReplanningRequiredEvent")
                
                replanning_event = ReplanningRequiredEvent(
                    task_id=task_id,
                    reason="æ“ä½œå¤±è´¥ï¼Œä¸”æ²¡æœ‰ç›´æ¥çš„ä¿®æ­£å»ºè®®ã€‚",
                    failure_details=reflection_result
                )
                await self.info_pool.publish_async(replanning_event)
        else:
            logger.info("âœ… æ“ä½œæˆåŠŸï¼Œæ— éœ€è¿›ä¸€æ­¥æ“ä½œã€‚")
    
    async def _assess_execution_quality(self, execution_result: Dict[str, Any], action_info: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè´¨é‡è¯„ä¼°"""
        # logger.info(f"ğŸ” å¼€å§‹æ‰§è¡Œè´¨é‡è¯„ä¼°...")
        # logger.info(f"åŸå§‹execution_result: {execution_result}")
        # logger.info(f"action_info: {action_info}")
        
        # ä»execution_resultä¸­æå–å®é™…çš„æ‰§è¡Œä¿¡æ¯
        execution_details = execution_result.get('execution_details', {})
        # logger.info(f"execution_details: {execution_details}")
        
        # å¤„ç†AgentResultå¯¹è±¡
        if hasattr(execution_details, 'success'):
            success = execution_details.success
            execution_time = getattr(execution_details, 'execution_time', 0)
            error_info = execution_details.error
            # logger.info(f"ä»AgentResultå¯¹è±¡æå–: success={success}, execution_time={execution_time}, error={error_info}")
        else:
            # ä»å­—å…¸ä¸­æå–
            success = execution_result.get("success", execution_details.get("success", False))
            execution_time = execution_result.get("execution_time", execution_details.get("execution_time", 0))
            error_info = execution_result.get("error", execution_details.get("error"))
            # logger.info(f"ä»å­—å…¸æå–: success={success}, execution_time={execution_time}, error={error_info}")
        
        # åŸºç¡€è´¨é‡æŒ‡æ ‡
        efficiency_score = self._calculate_efficiency_score_from_result(execution_result)
        reliability_score = self._calculate_reliability_score(execution_result)
        error_severity = self._assess_error_severity(error_info) if error_info else 0.0
        
        quality_metrics = {
            "success_rate": 1.0 if success else 0.0,
            "efficiency_score": efficiency_score,
            "reliability_score": reliability_score,
            "error_severity": error_severity
        }
        
        # logger.info(f"è´¨é‡æŒ‡æ ‡è¯¦æƒ…:")
        # logger.info(f"  - success_rate: {quality_metrics['success_rate']} (åŸºäºsuccess={success})")
        # logger.info(f"  - efficiency_score: {quality_metrics['efficiency_score']}")
        # logger.info(f"  - reliability_score: {quality_metrics['reliability_score']}")
        # logger.info(f"  - error_severity: {quality_metrics['error_severity']}")
        
        # ç»¼åˆè´¨é‡è¯„åˆ†
        overall_quality = (
            quality_metrics["success_rate"] * 0.4 +
            quality_metrics["efficiency_score"] * 0.3 +
            quality_metrics["reliability_score"] * 0.2 +
            (1.0 - quality_metrics["error_severity"]) * 0.1
        )
        
        assessment = "ä¼˜ç§€" if overall_quality > 0.8 else "è‰¯å¥½" if overall_quality > 0.6 else "éœ€æ”¹è¿›"
        
        # logger.info(f"ç»¼åˆè´¨é‡è¯„åˆ†è®¡ç®—:")
        # logger.info(f"  - å…¬å¼: {quality_metrics['success_rate']}*0.4 + {quality_metrics['efficiency_score']}*0.3 + {quality_metrics['reliability_score']}*0.2 + {1.0-quality_metrics['error_severity']}*0.1")
        # logger.info(f"  - ç»“æœ: {overall_quality:.3f} ({assessment})")
        
        result = {
            "overall_quality": overall_quality,
            "metrics": quality_metrics,
            "assessment": assessment
        }
        
        logger.info(f"âœ… æ‰§è¡Œè´¨é‡è¯„ä¼°å®Œæˆ: {result}")
        return result
    
    async def _identify_problems(self, execution_result: Dict[str, Any], action_info: Dict[str, Any]) -> Dict[str, Any]:
        """é—®é¢˜è¯†åˆ«"""
        # logger.info(f"ğŸ” å¼€å§‹é—®é¢˜è¯†åˆ«...")
        
        problems = []
        problem_categories = {
            "execution_errors": [],
            "performance_issues": [],
            "logic_problems": [],
            "environment_issues": []
        }
        
        # ä»execution_resultä¸­æå–å®é™…çš„æ‰§è¡Œä¿¡æ¯
        execution_details = execution_result.get('execution_details', {})
        
        # å¤„ç†AgentResultå¯¹è±¡
        if hasattr(execution_details, 'success'):
            success = execution_details.success
            execution_time = getattr(execution_details, 'execution_time', 0)
            error_info = execution_details.error
            retry_count = getattr(execution_details, 'retry_count', 0)
            # logger.info(f"ä»AgentResultæå–é—®é¢˜ä¿¡æ¯: success={success}, time={execution_time}, error={error_info}, retry={retry_count}")
        else:
            # ä»å­—å…¸ä¸­æå–
            success = execution_result.get("success", execution_details.get("success", False))
            execution_time = execution_result.get("execution_time", execution_details.get("execution_time", 0))
            error_info = execution_result.get("error", execution_details.get("error"))
            retry_count = execution_result.get("retry_count", execution_details.get("retry_count", 0))
            # logger.info(f"ä»å­—å…¸æå–é—®é¢˜ä¿¡æ¯: success={success}, time={execution_time}, error={error_info}, retry={retry_count}")
        
        # æ£€æŸ¥æ‰§è¡Œé”™è¯¯
        if not success:
            error_info = error_info or "æœªçŸ¥é”™è¯¯"
            problem_categories["execution_errors"].append({
                "type": "execution_failure",
                "description": f"æ‰§è¡Œå¤±è´¥: {error_info}",
                "severity": "high"
            })
            logger.warning(f"âŒ å‘ç°æ‰§è¡Œé”™è¯¯: {error_info}")
        else:
            logger.info(f"âœ… æ‰§è¡ŒæˆåŠŸï¼Œæ— æ‰§è¡Œé”™è¯¯")
        
        # æ£€æŸ¥æ€§èƒ½é—®é¢˜
        if execution_time is not None and execution_time > 10.0:  # è¶…è¿‡10ç§’è®¤ä¸ºæ˜¯æ€§èƒ½é—®é¢˜
            problem_categories["performance_issues"].append({
                "type": "slow_execution",
                "description": f"æ‰§è¡Œæ—¶é—´è¿‡é•¿: {execution_time:.2f}ç§’",
                "severity": "medium"
            })
            logger.warning(f"â±ï¸ å‘ç°æ€§èƒ½é—®é¢˜: æ‰§è¡Œæ—¶é—´{execution_time:.2f}ç§’")
        
        # æ£€æŸ¥é‡è¯•é—®é¢˜
        if retry_count is not None and retry_count > 0:
            problem_categories["logic_problems"].append({
                "type": "multiple_retries",
                "description": f"éœ€è¦é‡è¯•{retry_count}æ¬¡æ‰æˆåŠŸ",
                "severity": "medium"
            })
            logger.warning(f"ğŸ”„ å‘ç°é‡è¯•é—®é¢˜: é‡è¯•{retry_count}æ¬¡")
        
        # ç»Ÿè®¡é—®é¢˜æ€»æ•°
        total_problems = sum(len(problems) for problems in problem_categories.values())
        has_critical = any(
            problem.get("severity") == "high" 
            for category in problem_categories.values() 
            for problem in category
        )
        
        result = {
            "total_problems": total_problems,
            "problem_categories": problem_categories,
            "has_critical_issues": has_critical
        }
        
        # logger.info(f"é—®é¢˜è¯†åˆ«ç»“æœ:")
        # logger.info(f"  - æ€»é—®é¢˜æ•°: {total_problems}")
        # logger.info(f"  - ä¸¥é‡é—®é¢˜: {has_critical}")
        # logger.info(f"  - é—®é¢˜åˆ†ç±»: {problem_categories}")
        
        return result
    
    async def _perform_multimodal_analysis(self, before_screenshot: str, after_screenshot: str, 
                                         action_info: Dict[str, Any], expectation: str) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šæ¨¡æ€åˆ†æ"""
        logger.info(f"""ğŸ” å¼€å§‹å¤šæ¨¡æ€åˆ†æ...
åˆ†æå‚æ•°:
  - before_screenshot: {before_screenshot}
  - after_screenshot: {after_screenshot}
  - action_info: {action_info}
  - expectation: {expectation}""")
        
        try:
            # ä½¿ç”¨å¤šæ¨¡æ€åˆ†æå·¥å…·
            analysis_tool = self.get_tool("multimodal_action_analysis")
            if analysis_tool:
                action_data = {
                    "before_screenshot": before_screenshot,
                    "after_screenshot": after_screenshot,
                    "action": action_info,
                    "expectation": expectation,
                    "task_type": action_info.get("action", "unknown")
                }
                logger.info(f"è°ƒç”¨å¤šæ¨¡æ€åˆ†æå·¥å…·ï¼Œaction_data: {action_data}")
                result = await analysis_tool.aexecute(action_data=action_data)
                
                logger.info(f"""å¤šæ¨¡æ€åˆ†æç»“æœ:
  - success: {result.get('success')}
  - operation_success: {result.get('operation_success')}
  - outcome: {result.get('outcome')}
  - comparison_analysis: {result.get('comparison_analysis', '')[:100]}...
  - improvement_suggestions: {result.get('improvement_suggestions', '')[:100]}...""")
                
                return result
            else:
                logger.error(f"âŒ å¤šæ¨¡æ€åˆ†æå·¥å…·ä¸å¯ç”¨")
                return {"success": False, "error": "å¤šæ¨¡æ€åˆ†æå·¥å…·ä¸å¯ç”¨"}
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡æ€åˆ†æå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_improvement_suggestions(self, quality_assessment: Dict[str, Any], 
                                              problem_identification: Dict[str, Any],
                                              multimodal_analysis: Optional[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åŸºäºè´¨é‡è¯„ä¼°çš„å»ºè®®
        overall_quality = quality_assessment.get("overall_quality", 0.0)
        if overall_quality < 0.6:
            suggestions.append("æ•´ä½“æ‰§è¡Œè´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æ‰§è¡Œç­–ç•¥")
        
        metrics = quality_assessment.get("metrics", {})
        if metrics.get("efficiency_score", 1.0) < 0.7:
            suggestions.append("æ‰§è¡Œæ•ˆç‡æœ‰å¾…æå‡ï¼Œè€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–å‡å°‘ä¸å¿…è¦çš„æ­¥éª¤")
        
        # åŸºäºé—®é¢˜è¯†åˆ«çš„å»ºè®®
        problem_categories = problem_identification.get("problem_categories", {})
        
        if problem_categories.get("execution_errors"):
            suggestions.append("å­˜åœ¨æ‰§è¡Œé”™è¯¯ï¼Œå»ºè®®å¢å¼ºé”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ¢å¤æœºåˆ¶")
        
        if problem_categories.get("performance_issues"):
            suggestions.append("å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–æ‰§è¡Œæµç¨‹æˆ–å¢åŠ è¶…æ—¶å¤„ç†")
        
        if problem_categories.get("logic_problems"):
            suggestions.append("å­˜åœ¨é€»è¾‘é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ä»»åŠ¡åˆ†è§£å’Œæ‰§è¡Œé¡ºåº")
        
        # åŸºäºå¤šæ¨¡æ€åˆ†æçš„å»ºè®®
        if multimodal_analysis and multimodal_analysis.get("success"):
            if not multimodal_analysis.get("operation_success", True):
                suggestions.append("å¤šæ¨¡æ€åˆ†ææ˜¾ç¤ºæ“ä½œæœªè¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œå»ºè®®æ£€æŸ¥æ“ä½œç²¾åº¦")
            
            improvement_suggestions = multimodal_analysis.get("improvement_suggestions", "")
            if improvement_suggestions:
                suggestions.append(f"å¤šæ¨¡æ€åˆ†æå»ºè®®: {improvement_suggestions}")
        
        # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œæä¾›é€šç”¨å»ºè®®
        if not suggestions:
            suggestions.append("æ‰§è¡Œè¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰ç­–ç•¥")
        
        return suggestions
    
    async def _send_improvement_feedback_to_executor(self, reflection_result: Dict[str, Any], task_context: Dict[str, Any]) -> None:
        """å‘ExecutorAgentå‘é€å…·ä½“çš„æ”¹è¿›åé¦ˆ"""
        try:
            # åˆ†æåæ€ç»“æœï¼Œç”Ÿæˆå…·ä½“çš„æ”¹è¿›å»ºè®®
            improvement_suggestions = reflection_result.get("improvement_suggestions", "")
            multimodal_analysis = reflection_result.get("multimodal_analysis", {})
            
            # æå–åæ ‡ç›¸å…³çš„æ”¹è¿›å»ºè®®
            coordinate_feedback = self._extract_coordinate_feedback(multimodal_analysis, task_context)
            if coordinate_feedback:
                coordinate_event = Event(
                    type="execution_improvement_suggestion",
                    data={
                        "type": "coordinate_adjustment",
                        "content": "åŸºäºå¤šæ¨¡æ€åˆ†æçš„åæ ‡è°ƒæ•´å»ºè®®",
                        "coordinates": coordinate_feedback["original_coordinates"],
                        "adjustment": coordinate_feedback["suggested_adjustment"],
                        "reason": coordinate_feedback["reason"],
                        "confidence": coordinate_feedback.get("confidence", 0.7)
                    },
                    agent_id=self.config.id
                )
                await self.info_pool.publish_async(coordinate_event)
                logger.info(f"ğŸ“¤ å‘é€åæ ‡è°ƒæ•´å»ºè®®: {coordinate_feedback['suggested_adjustment']}")
            
            # æå–æ‰§è¡Œç­–ç•¥ç›¸å…³çš„æ”¹è¿›å»ºè®®
            strategy_feedback = self._extract_strategy_feedback(reflection_result, task_context)
            if strategy_feedback:
                strategy_event = Event(
                    type="execution_improvement_suggestion",
                    data={
                        "type": "execution_strategy",
                        "content": "åŸºäºåæ€åˆ†æçš„æ‰§è¡Œç­–ç•¥ä¼˜åŒ–",
                        "task_type": strategy_feedback["task_type"],
                        "strategy": strategy_feedback["strategy"],
                        "reason": strategy_feedback["reason"]
                    },
                    agent_id=self.config.id
                )
                await self.info_pool.publish_async(strategy_event)
                logger.info(f"ğŸ“¤ å‘é€ç­–ç•¥ä¼˜åŒ–å»ºè®®: {strategy_feedback['task_type']}")
            
            # å‘é€é€šç”¨æ”¹è¿›å»ºè®®
            if improvement_suggestions:
                general_event = Event(
                    type="execution_improvement_suggestion",
                    data={
                        "type": "general",
                        "content": improvement_suggestions,
                        "reflection_result": reflection_result,
                        "task_context": task_context
                    },
                    agent_id=self.config.id
                )
                await self.info_pool.publish_async(general_event)
                logger.info(f"ğŸ“¤ å‘é€é€šç”¨æ”¹è¿›å»ºè®®: {improvement_suggestions[:50]}...")
            
        except Exception as e:
            logger.error(f"âŒ å‘é€æ”¹è¿›åé¦ˆå¤±è´¥: {e}")
    
    def _extract_coordinate_feedback(self, multimodal_analysis: Dict[str, Any], task_context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»å¤šæ¨¡æ€åˆ†æä¸­æå–åæ ‡åé¦ˆ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒç´«è‰²ç‚¹æ ‡æ³¨åˆ†æ"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰åæ ‡ç›¸å…³çš„åˆ†æ
            analysis_text = str(multimodal_analysis)
            action_info = task_context.get("action_info", {})
            
            if "coordinates" not in action_info:
                return None
            
            original_coords = [action_info["coordinates"]["x"], action_info["coordinates"]["y"]]
            
            # å¢å¼ºçš„åæ ‡åˆ†æ - åŸºäºç´«è‰²ç‚¹æ ‡æ³¨å’Œè§†è§‰åˆ†æ
            coordinate_feedback = self._analyze_coordinate_precision(
                multimodal_analysis, original_coords, task_context
            )
            
            if coordinate_feedback:
                return coordinate_feedback
            
            # å›é€€åˆ°æ–‡æœ¬åˆ†æ
            if any(keyword in analysis_text for keyword in ["åæ ‡", "ä½ç½®", "åç§»", "åä¸Š", "åä¸‹", "åå·¦", "åå³", "ç´«è‰²ç‚¹", "æ ‡æ³¨"]):
                # åŸºäºåˆ†æå†…å®¹æ¨æ–­è°ƒæ•´æ–¹å‘
                if "åä¸Š" in analysis_text or "å¤ªé«˜" in analysis_text or "ä¸Šæ–¹" in analysis_text:
                    adjustment = [0, 15]  # å‘ä¸‹è°ƒæ•´
                    reason = "ç‚¹å‡»ä½ç½®åä¸Šï¼Œå»ºè®®å‘ä¸‹è°ƒæ•´"
                elif "åä¸‹" in analysis_text or "å¤ªä½" in analysis_text or "ä¸‹æ–¹" in analysis_text:
                    adjustment = [0, -15]  # å‘ä¸Šè°ƒæ•´
                    reason = "ç‚¹å‡»ä½ç½®åä¸‹ï¼Œå»ºè®®å‘ä¸Šè°ƒæ•´"
                elif "åå·¦" in analysis_text or "å·¦ä¾§" in analysis_text:
                    adjustment = [15, 0]  # å‘å³è°ƒæ•´
                    reason = "ç‚¹å‡»ä½ç½®åå·¦ï¼Œå»ºè®®å‘å³è°ƒæ•´"
                elif "åå³" in analysis_text or "å³ä¾§" in analysis_text:
                    adjustment = [-15, 0]  # å‘å·¦è°ƒæ•´
                    reason = "ç‚¹å‡»ä½ç½®åå³ï¼Œå»ºè®®å‘å·¦è°ƒæ•´"
                elif "ä¸å‡†ç¡®" in analysis_text or "åå·®" in analysis_text:
                    adjustment = [0, 0]  # éœ€è¦æ›´ç²¾ç¡®çš„åˆ†æ
                    reason = "åæ ‡å­˜åœ¨åå·®ï¼Œå»ºè®®é‡æ–°æ ¡å‡†"
                else:
                    return None
                
                return {
                    "original_coordinates": original_coords,
                    "suggested_adjustment": adjustment,
                    "reason": reason,
                    "confidence": 0.8
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"æå–åæ ‡åé¦ˆå¤±è´¥: {e}")
            return None
    
    def _analyze_coordinate_precision(self, multimodal_analysis: Dict[str, Any], 
                                    original_coords: List[int], task_context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """åˆ†æåæ ‡ç²¾åº¦ - åŸºäºç´«è‰²ç‚¹æ ‡æ³¨å’Œå¤šæ¨¡æ€åˆ†æ"""
        try:
            # ä»å¤šæ¨¡æ€åˆ†æç»“æœä¸­æå–åæ ‡ç²¾åº¦ä¿¡æ¯
            analysis_text = multimodal_analysis.get("comparison_analysis", "")
            improvement_suggestions = multimodal_analysis.get("improvement_suggestions", "")
            full_response = multimodal_analysis.get("full_response", "")
            
            # åˆå¹¶æ‰€æœ‰åˆ†ææ–‡æœ¬
            combined_text = f"{analysis_text} {improvement_suggestions} {full_response}"
            
            logger.info(f"ğŸ” åˆ†æåæ ‡ç²¾åº¦ï¼ŒåŸå§‹åæ ‡: {original_coords}")
            logger.info(f"ğŸ“ åˆ†ææ–‡æœ¬: {combined_text[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç´«è‰²ç‚¹æ ‡æ³¨ç›¸å…³çš„æè¿°
            if any(keyword in combined_text for keyword in ["ç´«è‰²ç‚¹", "æ ‡æ³¨", "ç‚¹å‡»ä½ç½®", "åç§»", "ä¸­å¿ƒ"]):
                return self._extract_precise_coordinate_adjustment(combined_text, original_coords)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„åƒç´ è°ƒæ•´å»ºè®®
            pixel_adjustment = self._extract_pixel_adjustment_from_text(combined_text)
            if pixel_adjustment:
                return {
                    "original_coordinates": original_coords,
                    "suggested_adjustment": pixel_adjustment["adjustment"],
                    "reason": pixel_adjustment["reason"],
                    "confidence": pixel_adjustment["confidence"],
                    "analysis_method": "text_pixel_extraction"
                }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–¹å‘æ€§è°ƒæ•´å»ºè®®
            directional_adjustment = self._extract_directional_adjustment(combined_text)
            if directional_adjustment:
                return {
                    "original_coordinates": original_coords,
                    "suggested_adjustment": directional_adjustment["adjustment"],
                    "reason": directional_adjustment["reason"],
                    "confidence": directional_adjustment["confidence"],
                    "analysis_method": "directional_analysis"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ åæ ‡ç²¾åº¦åˆ†æå¤±è´¥: {e}")
            return None
    
    def _extract_precise_coordinate_adjustment(self, analysis_text: str, original_coords: List[int]) -> Optional[Dict[str, Any]]:
        """ä»åˆ†ææ–‡æœ¬ä¸­æå–ç²¾ç¡®çš„åæ ‡è°ƒæ•´"""
        import re
        
        # æŸ¥æ‰¾å…·ä½“çš„åƒç´ è°ƒæ•´å»ºè®® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…é‡å¤åŒ¹é…
        adjustment_x = 0
        adjustment_y = 0
        reasons = []
        processed_matches = set()  # é¿å…é‡å¤å¤„ç†
        
        # æ°´å¹³æ–¹å‘è°ƒæ•´
        horizontal_patterns = [
            r'å‘å·¦è°ƒæ•´(\d+)åƒç´ ',
            r'å‘å³è°ƒæ•´(\d+)åƒç´ ',
            r'å·¦ç§»(\d+)åƒç´ ',
            r'å³ç§»(\d+)åƒç´ ',
            r'åå·¦(\d+)åƒç´ ',
            r'åå³(\d+)åƒç´ '
        ]
        
        for i, pattern in enumerate(horizontal_patterns):
            matches = re.findall(pattern, analysis_text)
            for match in matches:
                pixels = int(match)
                match_key = f"h_{i}_{pixels}"
                if match_key not in processed_matches:
                    processed_matches.add(match_key)
                    
                    if 'å·¦' in pattern:
                        adjustment_x -= pixels
                        reasons.append(f"å‘å·¦è°ƒæ•´{pixels}åƒç´ ")
                    else:
                        adjustment_x += pixels
                        reasons.append(f"å‘å³è°ƒæ•´{pixels}åƒç´ ")
        
        # å‚ç›´æ–¹å‘è°ƒæ•´
        vertical_patterns = [
            r'å‘ä¸Šè°ƒæ•´(\d+)åƒç´ ',
            r'å‘ä¸‹è°ƒæ•´(\d+)åƒç´ ',
            r'ä¸Šç§»(\d+)åƒç´ ',
            r'ä¸‹ç§»(\d+)åƒç´ ',
            r'åä¸Š(\d+)åƒç´ ',
            r'åä¸‹(\d+)åƒç´ '
        ]
        
        for i, pattern in enumerate(vertical_patterns):
            matches = re.findall(pattern, analysis_text)
            for match in matches:
                pixels = int(match)
                match_key = f"v_{i}_{pixels}"
                if match_key not in processed_matches:
                    processed_matches.add(match_key)
                    
                    if 'ä¸Š' in pattern:
                        adjustment_y -= pixels
                        reasons.append(f"å‘ä¸Šè°ƒæ•´{pixels}åƒç´ ")
                    else:
                        adjustment_y += pixels
                        reasons.append(f"å‘ä¸‹è°ƒæ•´{pixels}åƒç´ ")
        
        if adjustment_x != 0 or adjustment_y != 0:
            return {
                "original_coordinates": original_coords,
                "suggested_adjustment": [adjustment_x, adjustment_y],
                "reason": "åŸºäºå¤šæ¨¡æ€åˆ†æçš„ç²¾ç¡®è°ƒæ•´: " + ", ".join(set(reasons)),  # å»é‡
                "confidence": 0.9,
                "analysis_method": "precise_pixel_extraction"
            }
        
        return None
    
    def _extract_pixel_adjustment_from_text(self, text: str) -> Optional[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–åƒç´ çº§è°ƒæ•´å»ºè®®"""
        import re
        
        # æ›´å¤æ‚çš„åƒç´ è°ƒæ•´æ¨¡å¼
        patterns = [
            r'è°ƒæ•´åæ ‡.*?([+-]?\d+).*?([+-]?\d+)',
            r'åç§».*?([+-]?\d+).*?([+-]?\d+)',
            r'ç§»åŠ¨.*?([+-]?\d+).*?([+-]?\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    x_adj = int(match.group(1))
                    y_adj = int(match.group(2))
                    
                    return {
                        "adjustment": [x_adj, y_adj],
                        "reason": f"åŸºäºæ–‡æœ¬åˆ†æçš„åæ ‡è°ƒæ•´: X{x_adj:+d}, Y{y_adj:+d}",
                        "confidence": 0.7
                    }
                except ValueError:
                    continue
        
        return None
    
    def _extract_directional_adjustment(self, text: str) -> Optional[Dict[str, Any]]:
        """æå–æ–¹å‘æ€§è°ƒæ•´å»ºè®®"""
        # é»˜è®¤è°ƒæ•´åƒç´ æ•°
        default_pixels = 15
        
        adjustment_x = 0
        adjustment_y = 0
        reasons = []
        
        # æ£€æŸ¥æ–¹å‘æ€§æè¿°
        if any(word in text for word in ['åå·¦', 'å¤ªå·¦', 'å·¦ä¾§']):
            adjustment_x = default_pixels
            reasons.append("å‘å³è°ƒæ•´")
        elif any(word in text for word in ['åå³', 'å¤ªå³', 'å³ä¾§']):
            adjustment_x = -default_pixels
            reasons.append("å‘å·¦è°ƒæ•´")
        
        if any(word in text for word in ['åä¸Š', 'å¤ªé«˜', 'ä¸Šæ–¹']):
            adjustment_y = default_pixels
            reasons.append("å‘ä¸‹è°ƒæ•´")
        elif any(word in text for word in ['åä¸‹', 'å¤ªä½', 'ä¸‹æ–¹']):
            adjustment_y = -default_pixels
            reasons.append("å‘ä¸Šè°ƒæ•´")
        
        if adjustment_x != 0 or adjustment_y != 0:
            return {
                "adjustment": [adjustment_x, adjustment_y],
                "reason": "åŸºäºæ–¹å‘æ€§åˆ†æçš„è°ƒæ•´: " + ", ".join(reasons),
                "confidence": 0.6
            }
        
        return None
    
    def _extract_coordinate_feedback_from_analysis(self, coordinate_analysis: str, 
                                                 comparison_analysis: str, 
                                                 improvement_suggestions: str) -> Optional[Dict[str, Any]]:
        """ä»åˆ†ææ–‡æœ¬ä¸­æå–åæ ‡åé¦ˆä¿¡æ¯"""
        try:
            # åˆå¹¶æ‰€æœ‰ç›¸å…³æ–‡æœ¬
            combined_text = f"{coordinate_analysis} {comparison_analysis} {improvement_suggestions}"
            
            if not combined_text.strip():
                return None
            
            logger.info(f"ğŸ” ä»åˆ†ææ–‡æœ¬ä¸­æå–åæ ‡åé¦ˆ")
            logger.info(f"ğŸ“ åˆ†ææ–‡æœ¬: {combined_text[:150]}...")
            
            # å°è¯•æå–ç²¾ç¡®çš„åƒç´ è°ƒæ•´
            precise_adjustment = self._extract_precise_coordinate_adjustment(combined_text, [0, 0])
            if precise_adjustment:
                logger.info(f"âœ… æå–åˆ°ç²¾ç¡®è°ƒæ•´: {precise_adjustment['suggested_adjustment']}")
                return precise_adjustment
            
            # å°è¯•æå–åƒç´ çº§è°ƒæ•´
            pixel_adjustment = self._extract_pixel_adjustment_from_text(combined_text)
            if pixel_adjustment:
                logger.info(f"âœ… æå–åˆ°åƒç´ è°ƒæ•´: {pixel_adjustment['adjustment']}")
                return {
                    "original_coordinates": [0, 0],  # å ä½ç¬¦
                    "suggested_adjustment": pixel_adjustment["adjustment"],
                    "reason": pixel_adjustment["reason"],
                    "confidence": pixel_adjustment["confidence"],
                    "analysis_method": "coordinate_analysis_extraction"
                }
            
            # å°è¯•æå–æ–¹å‘æ€§è°ƒæ•´
            directional_adjustment = self._extract_directional_adjustment(combined_text)
            if directional_adjustment:
                logger.info(f"âœ… æå–åˆ°æ–¹å‘è°ƒæ•´: {directional_adjustment['adjustment']}")
                return {
                    "original_coordinates": [0, 0],  # å ä½ç¬¦
                    "suggested_adjustment": directional_adjustment["adjustment"],
                    "reason": directional_adjustment["reason"],
                    "confidence": directional_adjustment["confidence"],
                    "analysis_method": "coordinate_analysis_directional"
                }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç´«è‰²ç‚¹ç›¸å…³çš„æè¿°
            if any(keyword in combined_text for keyword in ["ç´«è‰²ç‚¹", "æ ‡æ³¨", "ç‚¹å‡»ä½ç½®", "ç›®æ ‡ä¸­å¿ƒ"]):
                # åŸºäºç´«è‰²ç‚¹æè¿°ç”Ÿæˆè°ƒæ•´å»ºè®®
                purple_dot_feedback = self._analyze_purple_dot_feedback(combined_text)
                if purple_dot_feedback:
                    logger.info(f"âœ… åŸºäºç´«è‰²ç‚¹åˆ†æ: {purple_dot_feedback['adjustment']}")
                    return {
                        "original_coordinates": [0, 0],  # å ä½ç¬¦
                        "suggested_adjustment": purple_dot_feedback["adjustment"],
                        "reason": purple_dot_feedback["reason"],
                        "confidence": purple_dot_feedback["confidence"],
                        "analysis_method": "purple_dot_analysis"
                    }
            
            logger.info(f"â„¹ï¸ æœªèƒ½ä»åˆ†ææ–‡æœ¬ä¸­æå–åæ ‡åé¦ˆ")
            return None
            
        except Exception as e:
            logger.error(f"âŒ æå–åæ ‡åé¦ˆå¤±è´¥: {e}")
            return None
    
    def _analyze_purple_dot_feedback(self, text: str) -> Optional[Dict[str, Any]]:
        """åˆ†æç´«è‰²ç‚¹ç›¸å…³çš„åé¦ˆ"""
        import re
        
        # æŸ¥æ‰¾ç´«è‰²ç‚¹ç›¸å…³çš„æè¿°
        purple_patterns = [
            r'ç´«è‰²ç‚¹.*?([åç¦»|è·ç¦»|è¿œç¦»]).*?([ä¸Šä¸‹å·¦å³]).*?(\d+)',
            r'ç‚¹å‡»ä½ç½®.*?([åç¦»|è·ç¦»]).*?ç›®æ ‡.*?([ä¸Šä¸‹å·¦å³]).*?(\d+)',
            r'æ ‡æ³¨.*?([åç¦»|è·ç¦»]).*?ä¸­å¿ƒ.*?([ä¸Šä¸‹å·¦å³]).*?(\d+)'
        ]
        
        for pattern in purple_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    direction = match.group(2)
                    distance = int(match.group(3))
                    
                    # æ ¹æ®æ–¹å‘è®¡ç®—è°ƒæ•´
                    adjustment_x = 0
                    adjustment_y = 0
                    
                    if direction == 'å·¦':
                        adjustment_x = distance
                    elif direction == 'å³':
                        adjustment_x = -distance
                    elif direction == 'ä¸Š':
                        adjustment_y = distance
                    elif direction == 'ä¸‹':
                        adjustment_y = -distance
                    
                    return {
                        "adjustment": [adjustment_x, adjustment_y],
                        "reason": f"åŸºäºç´«è‰²ç‚¹æ ‡æ³¨åˆ†æï¼Œç‚¹å‡»ä½ç½®å{direction}{distance}åƒç´ ",
                        "confidence": 0.85
                    }
                except (ValueError, IndexError):
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“æ•°å€¼ï¼Œä½¿ç”¨é»˜è®¤è°ƒæ•´
        if "ç´«è‰²ç‚¹" in text or "æ ‡æ³¨" in text:
            if any(word in text for word in ['åå·¦', 'å·¦ä¾§']):
                return {"adjustment": [20, 0], "reason": "ç´«è‰²ç‚¹æ˜¾ç¤ºåå·¦", "confidence": 0.6}
            elif any(word in text for word in ['åå³', 'å³ä¾§']):
                return {"adjustment": [-20, 0], "reason": "ç´«è‰²ç‚¹æ˜¾ç¤ºåå³", "confidence": 0.6}
            elif any(word in text for word in ['åä¸Š', 'ä¸Šæ–¹']):
                return {"adjustment": [0, 20], "reason": "ç´«è‰²ç‚¹æ˜¾ç¤ºåä¸Š", "confidence": 0.6}
            elif any(word in text for word in ['åä¸‹', 'ä¸‹æ–¹']):
                return {"adjustment": [0, -20], "reason": "ç´«è‰²ç‚¹æ˜¾ç¤ºåä¸‹", "confidence": 0.6}
        
        return None
    
    def _extract_strategy_feedback(self, reflection_result: Dict[str, Any], task_context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä»åæ€ç»“æœä¸­æå–ç­–ç•¥åé¦ˆ"""
        try:
            action_info = task_context.get("action_info", {})
            task_type = action_info.get("task_type", "unknown")
            
            # åŸºäºåæ€ç»“æœç”Ÿæˆç­–ç•¥å»ºè®®
            quality_assessment = reflection_result.get("quality_assessment", {})
            problem_identification = reflection_result.get("problem_identification", {})
            
            strategy = {}
            reason_parts = []
            
            # åŸºäºæ•ˆç‡é—®é¢˜è°ƒæ•´ç­–ç•¥
            efficiency_score = quality_assessment.get("metrics", {}).get("efficiency_score", 1.0)
            if efficiency_score < 0.5:
                strategy["timeout"] = 15.0  # å¢åŠ è¶…æ—¶æ—¶é—´
                strategy["retry_delay"] = 2.0  # å¢åŠ é‡è¯•å»¶è¿Ÿ
                reason_parts.append("æ•ˆç‡åä½")
            
            # åŸºäºé”™è¯¯ç±»å‹è°ƒæ•´ç­–ç•¥
            problem_categories = problem_identification.get("problem_categories", {})
            if problem_categories.get("performance_issues"):
                strategy["pre_wait"] = 1.0  # æ“ä½œå‰ç­‰å¾…
                reason_parts.append("æ€§èƒ½é—®é¢˜")
            
            if problem_categories.get("execution_errors"):
                strategy["verification_required"] = True  # éœ€è¦éªŒè¯
                reason_parts.append("æ‰§è¡Œé”™è¯¯")
            
            if strategy:
                return {
                    "task_type": task_type,
                    "strategy": strategy,
                    "reason": f"åŸºäº{', '.join(reason_parts)}çš„ç­–ç•¥ä¼˜åŒ–"
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"æå–ç­–ç•¥åé¦ˆå¤±è´¥: {e}")
            return None
    
    def _calculate_efficiency_score_from_result(self, execution_result: Dict[str, Any]) -> float:
        """ä»æ‰§è¡Œç»“æœè®¡ç®—æ•ˆç‡åˆ†æ•°"""
        # ä»execution_resultä¸­æå–å®é™…çš„æ‰§è¡Œä¿¡æ¯
        execution_details = execution_result.get('execution_details', {})
        
        # å¤„ç†AgentResultå¯¹è±¡
        if hasattr(execution_details, 'execution_time'):
            execution_time = getattr(execution_details, 'execution_time', 1.0)
            retry_count = getattr(execution_details, 'retry_count', 0)
        else:
            # ä»å­—å…¸ä¸­æå–
            execution_time = execution_result.get("execution_time", execution_details.get("execution_time", 1.0))
            retry_count = execution_result.get("retry_count", execution_details.get("retry_count", 0))
        
        # åŸºç¡€æ•ˆç‡åˆ†æ•°
        base_score = 1.0
        
        # æ ¹æ®æ‰§è¡Œæ—¶é—´è°ƒæ•´
        if execution_time is not None:
            if execution_time > 10.0:
                base_score *= 0.5
            elif execution_time > 5.0:
                base_score *= 0.7
            elif execution_time > 2.0:
                base_score *= 0.9
        
        # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´
        if retry_count is not None:
            base_score *= (1.0 - retry_count * 0.2)
        
        return max(0.0, min(1.0, base_score))
    
    def _calculate_reliability_score(self, execution_result: Dict[str, Any]) -> float:
        """è®¡ç®—å¯é æ€§åˆ†æ•°"""
        # ä»execution_resultä¸­æå–å®é™…çš„æ‰§è¡Œä¿¡æ¯
        execution_details = execution_result.get('execution_details', {})
        
        # å¤„ç†AgentResultå¯¹è±¡
        if hasattr(execution_details, 'success'):
            success = execution_details.success
            error_info = execution_details.error
        else:
            # ä»å­—å…¸ä¸­æå–
            success = execution_result.get("success", execution_details.get("success", False))
            error_info = execution_result.get("error", execution_details.get("error"))
        
        if success:
            return 1.0
        elif error_info:
            # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´å¯é æ€§åˆ†æ•°
            error_str = str(error_info).lower()
            if "timeout" in error_str:
                return 0.3
            elif "permission" in error_str:
                return 0.2
            elif "network" in error_str:
                return 0.4
            else:
                return 0.1
        else:
            return 0.0
    
    def _assess_error_severity(self, error_info: str) -> float:
        """è¯„ä¼°é”™è¯¯ä¸¥é‡ç¨‹åº¦ (0.0-1.0, 1.0ä¸ºæœ€ä¸¥é‡)"""
        error_str = str(error_info).lower()
        
        if any(keyword in error_str for keyword in ["crash", "fatal", "critical"]):
            return 1.0
        elif any(keyword in error_str for keyword in ["error", "failed", "exception"]):
            return 0.7
        elif any(keyword in error_str for keyword in ["warning", "timeout"]):
            return 0.4
        else:
            return 0.2
    
    def _calculate_overall_score(self, quality_assessment: Dict[str, Any], 
                               problem_identification: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        logger.info(f"ğŸ” å¼€å§‹è®¡ç®—æ€»ä½“è¯„åˆ†...")
        
        quality_score = quality_assessment.get("overall_quality", 0.0)
        problem_count = problem_identification.get("total_problems", 0)
        has_critical = problem_identification.get("has_critical_issues", False)
        
        logger.info(f"è¯„åˆ†è®¡ç®—è¾“å…¥:")
        logger.info(f"  - è´¨é‡åˆ†æ•°: {quality_score}")
        logger.info(f"  - é—®é¢˜æ•°é‡: {problem_count}")
        logger.info(f"  - ä¸¥é‡é—®é¢˜: {has_critical}")
        
        # åŸºç¡€åˆ†æ•°æ¥è‡ªè´¨é‡è¯„ä¼°
        overall_score = quality_score
        logger.info(f"åŸºç¡€åˆ†æ•°: {overall_score}")
        
        # æ ¹æ®é—®é¢˜æ•°é‡è°ƒæ•´
        problem_penalty = problem_count * 0.1
        overall_score *= (1.0 - problem_penalty)
        logger.info(f"é—®é¢˜è°ƒæ•´å: {overall_score} (é—®é¢˜æƒ©ç½š: {problem_penalty})")
        
        # å¦‚æœæœ‰ä¸¥é‡é—®é¢˜ï¼Œå¤§å¹…é™ä½åˆ†æ•°
        if has_critical:
            overall_score *= 0.5
            logger.warning(f"ä¸¥é‡é—®é¢˜æƒ©ç½šå: {overall_score}")
        
        final_score = max(0.0, min(1.0, overall_score))
        
        logger.info(f"æœ€ç»ˆè¯„åˆ†: {final_score:.3f}")
        logger.info(f"è¯„åˆ†ç­‰çº§: {'ä¼˜ç§€' if final_score > 0.8 else 'è‰¯å¥½' if final_score > 0.6 else 'éœ€æ”¹è¿›'}")
        
        return final_score
    
    async def _comprehensive_multimodal_analysis(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """ç»¼åˆå¤šæ¨¡æ€åˆ†æ - ç»“åˆå¤šç§åˆ†ææ–¹æ³•"""
        # æ‰§è¡Œå¤šæ¨¡æ€åæ€åˆ†æ
        multimodal_result = await self._multimodal_action_reflection(task_context)
        
        # è·å–å‚æ•°
        action_history = task_context.get("action_history", [])
        analysis_results = task_context.get("analysis_results", [])
        
        # æ‰§è¡Œæ€§èƒ½åˆ†æ
        performance_result = {"success": False, "error": "æœªæ‰§è¡Œæ€§èƒ½åˆ†æ"}
        performance_tool = self.get_tool("performance_analysis")
        if performance_tool:
            result = performance_tool.execute(action_history=action_history)
        else:
            result = {"success": False, "error": "æœªæ‰¾åˆ°æ€§èƒ½åˆ†æå·¥å…·"}
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = result
        
        # ç”Ÿæˆå­¦ä¹ æ´å¯Ÿ
        insight_result = {"success": False, "error": "æœªæ‰§è¡Œå­¦ä¹ æ´å¯Ÿ"}
        insight_tool = self.get_tool("learning_insight")
        if insight_tool:
            result = insight_tool.execute(analysis_results=analysis_results)
        else:
            result = {"success": False, "error": "æœªæ‰¾åˆ°å­¦ä¹ æ´å¯Ÿå·¥å…·"}
        
        # æ›´æ–°å­¦ä¹ æ´å¯Ÿ
        self.learning_insights = result
        
        logger.info(f"å­¦ä¹ æ´å¯Ÿç”Ÿæˆå®Œæˆ: {len(analysis_results)}ä¸ªåˆ†æç»“æœ")
        return result
    
    def _generate_comprehensive_summary(
        self, 
        multimodal_result: Dict[str, Any], 
        performance_result: Dict[str, Any], 
        insight_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†ææ‘˜è¦"""
        return {
            "operation_success": multimodal_result.get("operation_success", False),
            "outcome_category": multimodal_result.get("outcome", "unknown"),
            "model_used": multimodal_result.get("model_used", "unknown"),
            "analysis_method": "multimodal_llm_reflection",
            "performance_score": performance_result.get("success_rate", 0.0),
            "insights_generated": len(insight_result.get("key_learnings", [])),
            "recommendations_count": len(multimodal_result.get("improvement_suggestions", "")),
            "analysis_quality": "high" if multimodal_result.get("success") else "limited"
        }
    
    async def _analyze_single_action(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªåŠ¨ä½œ"""
        action_data = task_context.get("action_data", {})
        
        analysis_tool = self.get_tool("action_analysis")
        if analysis_tool:
            result = analysis_tool.execute(action_data=action_data)
        else:
            result = {"success": False, "error": "æœªæ‰¾åˆ°åˆ†æå·¥å…·"}
        
        logger.info(f"å•ä¸ªåŠ¨ä½œåˆ†æå®Œæˆ: {action_data.get('task_type', 'unknown')}")
        return result
    
    async def _analyze_performance(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•´ä½“æ€§èƒ½"""
        action_history = task_context.get("action_history", [])
        
        performance_tool = self.get_tool("performance_analysis")
        if performance_tool:
            result = performance_tool.execute(action_history=action_history)
        else:
            result = {"success": False, "error": "æœªæ‰¾åˆ°æ€§èƒ½åˆ†æå·¥å…·"}
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = result
        
        logger.info(f"æ€§èƒ½åˆ†æå®Œæˆ: {len(action_history)}ä¸ªåŠ¨ä½œ")
        return result
    
    async def _generate_learning_insights(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå­¦ä¹ æ´å¯Ÿ"""
        analysis_results = task_context.get("analysis_results", self.reflection_history)
        
        insight_tool = self.get_tool("learning_insight")
        if insight_tool:
            result = insight_tool.execute(analysis_results=analysis_results)
        else:
            result = {"success": False, "error": "æœªæ‰¾åˆ°å­¦ä¹ æ´å¯Ÿå·¥å…·"}
        
        # æ›´æ–°å­¦ä¹ æ´å¯Ÿ
        self.learning_insights = result
        
        logger.info(f"å­¦ä¹ æ´å¯Ÿç”Ÿæˆå®Œæˆ: {len(analysis_results)}ä¸ªåˆ†æç»“æœ")
        return result
    
    async def _comprehensive_analysis(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æ"""
        action_history = task_context.get("action_history", [])
        
        # æ‰§è¡Œæ€§èƒ½åˆ†æ
        performance_result = await self._analyze_performance({"action_history": action_history})
        
        # åˆ†ææ¯ä¸ªåŠ¨ä½œ
        action_analyses = []
        for action in action_history[-10:]:  # åªåˆ†ææœ€è¿‘10ä¸ªåŠ¨ä½œ
            try:
                analysis = await self._analyze_single_action({"action_data": action})
                action_analyses.append(analysis)
            except Exception as e:
                logger.warning(f"åŠ¨ä½œåˆ†æå¤±è´¥: {e}")
        
        # ç”Ÿæˆå­¦ä¹ æ´å¯Ÿ
        insight_result = await self._generate_learning_insights({"analysis_results": action_analyses})
        
        comprehensive_result = {
            "performance_analysis": performance_result,
            "action_analyses": action_analyses,
            "learning_insights": insight_result,
            "summary": self._generate_analysis_summary(performance_result, insight_result),
            "analysis_time": get_iso_timestamp()
        }
        
        logger.info("ç»¼åˆåˆ†æå®Œæˆ")
        return comprehensive_result
    
    def _generate_analysis_summary(self, performance_result: Dict[str, Any], insight_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        return {
            "total_actions_analyzed": performance_result.get("total_actions", 0),
            "overall_success_rate": performance_result.get("success_rate", 0.0),
            "overall_efficiency": performance_result.get("average_efficiency", 0.0),
            "key_insights_count": len(insight_result.get("key_learnings", [])),
            "improvement_suggestions_count": len(insight_result.get("improvement_opportunities", [])),
            "analysis_quality": "good" if performance_result.get("success_rate", 0) > 0.8 else "needs_improvement"
        }
    
    def _handle_action_result(self, info_entry) -> None:
        """å¤„ç†åŠ¨ä½œç»“æœä¿¡æ¯ - è§¦å‘å¤šæ¨¡æ€åæ€åˆ†æ"""
        try:
            logger.info(f"ğŸ” [ACTION_RESULT_HANDLER] Received info_entry: {info_entry}")
            if hasattr(info_entry, 'data'):
                logger.info(f"ğŸ” [ACTION_RESULT_HANDLER] info_entry.data: {info_entry.data}")
            else:
                logger.warning(f"ğŸ” [ACTION_RESULT_HANDLER] info_entry has no 'data' attribute.")
            
            action_record = info_entry.data.get("action_record", {})
            
            logger.info(f"ğŸ“¨ æ”¶åˆ°åŠ¨ä½œç»“æœ: {action_record.get('task_type', 'unknown')}")
            
            # å¦‚æœæœ‰æ“ä½œå‰åæˆªå›¾ï¼Œè§¦å‘å¤šæ¨¡æ€åæ€åˆ†æ
            if self._should_trigger_reflection(action_record):
                asyncio.create_task(self._trigger_reflection_analysis(action_record))
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†åŠ¨ä½œç»“æœå¤±è´¥: {e}", exc_info=True)
    
    def _handle_screenshot_taken(self, info_entry) -> None:
        """å¤„ç†æˆªå›¾äº‹ä»¶"""
        try:
            screenshot_data = info_entry.data
            screenshot_path = screenshot_data.get("screenshot_path")
            screenshot_type = screenshot_data.get("type", "unknown")  # before/after
            
            logger.info(f"ğŸ“¸ æ”¶åˆ°æˆªå›¾äº‹ä»¶: {screenshot_type} - {screenshot_path}")
            
            # å­˜å‚¨æˆªå›¾ä¿¡æ¯ç”¨äºåç»­åˆ†æ
            if screenshot_type in ["before", "after"]:
                self._store_screenshot_info(screenshot_path, screenshot_type)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æˆªå›¾äº‹ä»¶å¤±è´¥: {e}")
    
    def _handle_operation_completed(self, info_entry) -> None:
        """å¤„ç†æ“ä½œå®Œæˆäº‹ä»¶ - è‡ªåŠ¨è§¦å‘åæ€åˆ†æ"""
        try:
            operation_data = info_entry.data
            
            logger.info(f"âœ… æ”¶åˆ°æ“ä½œå®Œæˆäº‹ä»¶: {operation_data.get('operation_type', 'unknown')}")
            
            # è‡ªåŠ¨è§¦å‘åæ€åˆ†æ
            asyncio.create_task(self._auto_reflection_analysis(operation_data))
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ“ä½œå®Œæˆäº‹ä»¶å¤±è´¥: {e}")
    
    def _should_trigger_reflection(self, action_record: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘åæ€åˆ†æ"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ä¿¡æ¯
        task_type = action_record.get("task_type")
        
        # å¯¹äºè¿™äº›æ“ä½œç±»å‹ï¼Œéœ€è¦è¿›è¡Œåæ€åˆ†æ
        reflection_worthy_actions = [
            "click_action", "input_text", "swipe_action", 
            "long_press_action", "open_app_action"
        ]
        
        return task_type in reflection_worthy_actions
    
    def _store_screenshot_info(self, screenshot_path: str, screenshot_type: str) -> None:
        """å­˜å‚¨æˆªå›¾ä¿¡æ¯"""
        # ç®€åŒ–çš„æˆªå›¾å­˜å‚¨é€»è¾‘
        # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é…å¯¹é€»è¾‘
        if not hasattr(self, '_temp_screenshots'):
            self._temp_screenshots = {}
        
        self._temp_screenshots[screenshot_type] = screenshot_path
        
        # å¦‚æœæœ‰æ“ä½œå‰åæˆªå›¾å¯¹ï¼Œå­˜å‚¨åˆ°å†å²ä¸­
        if 'before' in self._temp_screenshots and 'after' in self._temp_screenshots:
            self.screenshot_pairs.append((
                self._temp_screenshots['before'],
                self._temp_screenshots['after']
            ))
            # ä¿æŒåˆç†çš„å†å²è®°å½•æ•°é‡
            if len(self.screenshot_pairs) > 50:
                self.screenshot_pairs = self.screenshot_pairs[-50:]
            
            # æ¸…ç©ºä¸´æ—¶å­˜å‚¨
            self._temp_screenshots = {}
    
    async def _trigger_reflection_analysis(self, action_record: Dict[str, Any]) -> None:
        """è§¦å‘åæ€åˆ†æ"""
        try:
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            task_context = {
                "analysis_type": "multimodal_reflection",
                "action_info": action_record,
                "expectation": action_record.get("expectation", "æ“ä½œæˆåŠŸå®Œæˆ"),
            }
            
            # ä» action_record ä¸­æå–æˆªå›¾ä¿¡æ¯
            action_result = action_record.get("result", {})
            before_screenshot = action_result.get("before_screenshot")
            after_screenshot = action_result.get("after_screenshot")

            # å¦‚æœåœ¨ action_record ä¸­æ‰¾åˆ°æˆªå›¾ï¼Œåˆ™ä½¿ç”¨å®ƒä»¬
            if before_screenshot and after_screenshot:
                logger.info(f"ä» action_record ä¸­æ‰¾åˆ°æˆªå›¾: before='{before_screenshot}', after='{after_screenshot}'")
                task_context.update({
                    "before_screenshot": before_screenshot,
                    "after_screenshot": after_screenshot
                })
            # å›é€€åˆ°ä½¿ç”¨ screenshot_pairs
            elif self.screenshot_pairs:
                logger.warning("åœ¨ action_record ä¸­æœªæ‰¾åˆ°æˆªå›¾ï¼Œå›é€€åˆ°ä½¿ç”¨ screenshot_pairs")
                before_screenshot, after_screenshot = self.screenshot_pairs[-1]
                task_context.update({
                    "before_screenshot": before_screenshot,
                    "after_screenshot": after_screenshot
                })
            
            # æ‰§è¡Œåæ€åˆ†æ
            result = await self._execute_task_impl(task_context)
            
            logger.info(f"ğŸ” è‡ªåŠ¨åæ€åˆ†æå®Œæˆ: {result.get('operation_success', 'unknown')}")
            
        except Exception as e:
            logger.error(f"âŒ è§¦å‘åæ€åˆ†æå¤±è´¥: {e}")
    
    async def _auto_reflection_analysis(self, operation_data: Dict[str, Any]) -> None:
        """è‡ªåŠ¨åæ€åˆ†æ"""
        try:
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            task_context = {
                "analysis_type": "multimodal_reflection",
                "action_info": operation_data,
                "expectation": operation_data.get("expected_outcome", "æ“ä½œæŒ‰é¢„æœŸå®Œæˆ"),
                "before_screenshot": operation_data.get("before_screenshot"),
                "after_screenshot": operation_data.get("after_screenshot")
            }
            
            # æ‰§è¡Œåæ€åˆ†æ
            result = await self._execute_task_impl(task_context)
            
            logger.info(f"ğŸ¤– è‡ªåŠ¨åæ€åˆ†æå®Œæˆ: æ“ä½œ{'æˆåŠŸ' if result.get('operation_success') else 'å¤±è´¥'}")
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨åæ€åˆ†æå¤±è´¥: {e}")
    
    def get_reflection_history(self) -> List[Dict[str, Any]]:
        """è·å–åæ€åˆ†æå†å²"""
        return self.reflection_history.copy()
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.performance_metrics.copy()
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ æ´å¯Ÿ"""
        return self.learning_insights.copy()
    
    def get_screenshot_pairs(self) -> List[Tuple[str, str]]:
        """è·å–æˆªå›¾å¯¹å†å²"""
        return self.screenshot_pairs.copy()
    
    def clear_reflection_history(self) -> None:
        """æ¸…ç©ºåæ€å†å²"""
        self.reflection_history.clear()
        self.performance_metrics.clear()
        self.learning_insights.clear()
        self.screenshot_pairs.clear()
        if hasattr(self, '_temp_screenshots'):
            self._temp_screenshots.clear()
        logger.info("ğŸ§¹ åæ€åˆ†æå†å²å·²æ¸…ç©º")
    
    async def manual_reflection_analysis(
        self, 
        before_screenshot: str, 
        after_screenshot: str, 
        action_info: Dict[str, Any], 
        expectation: str = ""
    ) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘åæ€åˆ†æ - ä¾¿æ·æ–¹æ³•
        
        Args:
            before_screenshot: æ“ä½œå‰æˆªå›¾è·¯å¾„
            after_screenshot: æ“ä½œåæˆªå›¾è·¯å¾„
            action_info: åŠ¨ä½œä¿¡æ¯
            expectation: æœŸæœ›ç»“æœ
        
        Returns:
            åæ€åˆ†æç»“æœ
        """
        task_context = {
            "analysis_type": "multimodal_reflection",
            "before_screenshot": before_screenshot,
            "after_screenshot": after_screenshot,
            "action_info": action_info,
            "expectation": expectation
        }
        
        return await self._execute_task_impl(task_context)
    
    def get_recent_reflections(self, count: int = 5) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„åæ€åˆ†æç»“æœ"""
        return self.reflection_history[-count:] if self.reflection_history else []
    
    def get_success_rate(self) -> float:
        """è·å–æ“ä½œæˆåŠŸç‡"""
        if not self.reflection_history:
            return 0.0
        
        successful_operations = sum(
            1 for reflection in self.reflection_history 
            if reflection.get("result", {}).get("operation_success", False)
        )
        
        return successful_operations / len(self.reflection_history)
    
    def get_model_usage_stats(self) -> Dict[str, int]:
        """è·å–æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡"""
        model_stats = defaultdict(int)
        
        for reflection in self.reflection_history:
            model_used = reflection.get("model_used", "unknown")
            model_stats[model_used] += 1
        
        return dict(model_stats)